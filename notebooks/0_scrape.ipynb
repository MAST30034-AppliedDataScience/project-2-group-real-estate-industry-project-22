{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape\n",
    "\n",
    "**Jupyter notebook that scrapes domain.com.au and outputs the information collected into parquet files**\n",
    "\n",
    "Below are some functions that will help with scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(suburbs_df) -> dict:\n",
    "    \"\"\"function that splits up postcode dataframe into chunks of 25 so that if we are kicked halfway during scraping we don't lose too much progress\n",
    "\n",
    "    parameters: \n",
    "    suburbs_df -> spark dataframe containing postcode data which we are wanting to split up into chunks to assist with scraping\n",
    "\n",
    "    output:\n",
    "    chunk_dict -> dictionary containing chunked up postcodes which will be used when we scrape our data\n",
    "    \"\"\"\n",
    "    chunk_dict = {}\n",
    "    \n",
    "    i = 3048\n",
    "    j = 3023  \n",
    "    while i < 3997:\n",
    "        temp = suburbs_df[suburbs_df['postcode'] > j]\n",
    "        chunk_dict['chunk_{}'.format(i)] = temp[temp['postcode'] <= i]\n",
    "        j += 25\n",
    "        i += 25\n",
    "\n",
    "    return chunk_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working METHOD\n",
    "import re\n",
    "from json import dump\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.master('local[*]') \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .appName(\"PropertyScraper\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://www.domain.com.au\"\n",
    "N_PAGES = range(1, 50)  # Max number of pages you want to scrape  \n",
    "\n",
    "# Load suburbs CSV\n",
    "suburbs_df = pd.read_csv('postcodes.csv')  # Ensure this CSV contains 'suburb' and 'postcode' columns\n",
    "chunk_dict = get_chunks(suburbs_df)\n",
    "\n",
    "def start_scrape(chunk: dict, file_suffix: int) -> None:\n",
    "    \"\"\"Function that scrapes https://www.domain.com.au and outputs the data into a JSON file\n",
    "    \n",
    "    parameters:\n",
    "    chunk: chunk of 50 postcodes we will scrape\n",
    "    file_suffix: what we want to title the end of our files when we write to parquet (eg. chunk_{file_suffix}.parquet)\n",
    "\n",
    "    output:\n",
    "    No output, \n",
    "    \"\"\"\n",
    "\n",
    "    # Define schema for the Spark DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"url\", StringType(), True),\n",
    "        StructField(\"postcode\", StringType(), True),\n",
    "        StructField(\"suburb\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"cost_text\", StringType(), True),\n",
    "        StructField(\"beds\", StringType(), True),  # Separate field for beds\n",
    "        StructField(\"baths\", StringType(), True),  # Separate field for baths\n",
    "        StructField(\"parking\", StringType(), True),  # Parking field\n",
    "        StructField(\"property_type\", StringType(), True),  # Property type field\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Initialize an empty DataFrame with the schema\n",
    "    property_metadata = spark.createDataFrame([], schema)\n",
    "\n",
    "    # Loop through each suburb and its postcode\n",
    "    for index, row in chunk.iterrows():\n",
    "        suburb = row['locality'].lower().replace(' ', '-')  # Convert to lowercase and hyphenate\n",
    "        postcode = row['postcode']\n",
    "\n",
    "        print(f\"Scraping data for {suburb} ({postcode})\")\n",
    "\n",
    "        url_links = []\n",
    "        page_found = False  # This flag will help us track whether any results are found\n",
    "\n",
    "        # Generate list of URLs to visit\n",
    "        for page in N_PAGES:\n",
    "            url = BASE_URL + f\"/rent/{suburb}-vic-{postcode}/?ssubs=0&sort=suburb-asc&page={page}\"\n",
    "            try:\n",
    "                bs_object = BeautifulSoup(urlopen(Request(url, headers={'User-Agent': \"PostmanRuntime/7.6.0\"})), \"lxml\")\n",
    "\n",
    "                # Check if the page has results or shows \"No results found\"\n",
    "                no_results = bs_object.find(text=re.compile(\"No results found\", re.I))\n",
    "                if no_results:\n",
    "                    print(f\"No results found for {suburb} on page {page}. Stopping further scraping for this suburb.\")\n",
    "                    break  # Exit the pagination loop if no results are found\n",
    "\n",
    "                # Find property links\n",
    "                index_links = bs_object.find(\"ul\", {\"data-testid\": \"results\"})\n",
    "                if not index_links:\n",
    "                    print(f\"No more results for {suburb} on page {page}.\")\n",
    "                    break  # Exit pagination if no results list is found (end of pages)\n",
    "\n",
    "                index_links = index_links.findAll(\"a\", href=re.compile(f\"{BASE_URL}/*\"))\n",
    "                page_found = True  # At least one result was found on this page\n",
    "\n",
    "                for link in index_links:\n",
    "                    # If it's a property address, add it to the list\n",
    "                    if 'address' in link.get('class', []):\n",
    "                        url_links.append(link['href'])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {url}: {e}\")\n",
    "                break  # Stop if there's an issue with fetching the page\n",
    "\n",
    "        if not page_found:\n",
    "            print(f\"No results for {suburb}. Moving to the next suburb.\")\n",
    "            continue  # Skip to the next suburb if no pages were found for this one\n",
    "\n",
    "        # For each URL, scrape some basic metadata\n",
    "        pbar = tqdm(url_links)\n",
    "        success_count, total_count = 0, 0\n",
    "\n",
    "        for property_url in pbar:\n",
    "            try:\n",
    "                bs_object = BeautifulSoup(urlopen(Request(property_url, headers={'User-Agent': \"PostmanRuntime/7.6.0\"})), \"lxml\")\n",
    "                total_count += 1\n",
    "\n",
    "                # Get property name\n",
    "                name = bs_object.find(\"h1\", {\"class\": \"css-164r41r\"}).text.strip()\n",
    "\n",
    "                # Get cost text\n",
    "                cost_text = bs_object.find(\"div\", {\"data-testid\": \"listing-details__summary-title\"}).text.strip()\n",
    "\n",
    "                # Get rooms (beds and baths)\n",
    "                rooms = bs_object.find(\"div\", {\"data-testid\": \"property-features\"}).findAll(\n",
    "                    \"span\", {\"data-testid\": \"property-features-text-container\"}\n",
    "                )\n",
    "\n",
    "                # Initialize variables\n",
    "                beds, baths, parking = None, None, '0'  # Default value for parking is '0 Car'\n",
    "\n",
    "                for feature in rooms:\n",
    "                    text = feature.text\n",
    "                    if 'Bed' in text:\n",
    "                        beds_match = re.findall(r'\\d+', text)\n",
    "                        if beds_match:\n",
    "                            beds = beds_match[0]  # Extract the number of beds\n",
    "                    elif 'Bath' in text:\n",
    "                        baths_match = re.findall(r'\\d+', text)\n",
    "                        if baths_match:\n",
    "                            baths = baths_match[0]  # Extract the number of baths\n",
    "                    elif 'Car' in text or 'Parking' in text:\n",
    "                        parking_match = re.findall(r'\\d+', text)\n",
    "                        if parking_match:\n",
    "                            parking = parking_match[0]  # Extract the number of parking spaces\n",
    "\n",
    "                property_type_container = bs_object.find(\"div\", {\"data-testid\": \"listing-summary-property-type\"})\n",
    "                property_type = property_type_container.get_text(strip=True)\n",
    "\n",
    "                # Create a row and append it to the DataFrame\n",
    "                row = [(property_url, postcode, suburb, name, cost_text, beds, baths, parking, property_type)]\n",
    "                row_df = spark.createDataFrame(row, schema)\n",
    "                property_metadata = property_metadata.union(row_df)\n",
    "                success_count += 1\n",
    "\n",
    "            except AttributeError:\n",
    "                print(f\"Error scraping {property_url}: missing data\")\n",
    "\n",
    "            pbar.set_description(f\"{(success_count / total_count * 100):.0f}% successful\")\n",
    "\n",
    "        # Show the DataFrame to ensure data is being appended\n",
    "        #property_metadata.show()\n",
    "\n",
    "    # Output to parquet file\n",
    "    try:\n",
    "        property_metadata.write.mode(\"overwrite\").parquet('../data/raw/work_{}.parquet'.format(file_suffix))\n",
    "        print(f\"Data successfully written\")\n",
    "    except Exception as e:\n",
    "       print(f\"An error occured: {e}\")\n",
    "\n",
    "    #added this print statement so that the cell output can be scrollable - it's getting annoying to click the scroll bar >:(\n",
    "    print(\"chunk finished\")\n",
    "    #return property_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chunk(starting_chunk: int) -> None:\n",
    "    \"\"\"Function that scrapes domain.com.au in chunks of 25 postcodes 7 times (split amongst group members)\n",
    "    \n",
    "    Parameters:\n",
    "    starting_chunk -> starting chunk number that we want\n",
    "\n",
    "    Return:\n",
    "    None \n",
    "    \"\"\"\n",
    "    i = starting_chunk\n",
    "    # we are running chunks of 25 postcodes 7 times each\n",
    "    while i < starting_chunk + 175:\n",
    "        start_scrape(chunk_dict[\"chunk_{}\".format(i)], i) #i.split(\"_\")[1])\n",
    "        i += 25\n",
    "    if i == 3923:\n",
    "        start_scrape(chunk_dict[\"chunk_3923\"], 3923)\n",
    "        temp = suburbs_df[suburbs_df['postcode'] >= i + 1]\n",
    "        chunk_dict['chunk_3996'] = temp[temp['postcode'] < 3997]\n",
    "        start_scrape(chunk_dict['chunk_3996'], 3996)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Down below we split up the scraping process between team members to scrape more efficiently and minimise loss of time if hardware limitations occur**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Davyn\n",
    "starting_chunk = 3148\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arpan\n",
    "starting_chunk = 3048 + 175\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Priscilla\n",
    "starting_chunk = 3048 + 350\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rachel\n",
    "starting_chunk = 3048 + 525\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nathan\n",
    "starting_chunk = 3048 + 700\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to scrape the chunk that includes malvern separately as we incur connection errors whenever we try access malvern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape for suburbs in this chunk except for malvern as the connection breaks \n",
    "temp = suburbs_df[suburbs_df['postcode'] >= 3123]\n",
    "chunk_dict['chunk_3148'] = temp[temp['postcode'] <= 3148]\n",
    "start_scrape(chunk_dict['chunk_3148'], 3148)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
