{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "Picked up _JAVA_OPTIONS: -Xmx2048m\n",
      "Picked up _JAVA_OPTIONS: -Xmx2048m\n",
      "24/10/17 21:41:53 WARN Utils: Your hostname, Rachel resolves to a loopback address: 127.0.1.1; using 172.20.96.36 instead (on interface eth0)\n",
      "24/10/17 21:41:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/17 21:41:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Liveability\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config('spark.driver.memory', '4g')\n",
    "    .config('spark.executor.memory', '2g')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/home/rachel/Assignment 2/project-2-group-real-estate-industry-project-22/data/postcodes/postcodes.parquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/postcodes/postcodes.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    542\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/home/rachel/Assignment 2/project-2-group-real-estate-industry-project-22/data/postcodes/postcodes.parquet."
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"../data/postcodes/postcodes.parquet\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Google Places API key\n",
    "API_KEY = 'AIzaSyDKBch72s8hyaVK4GsnrOhA5AnWT4IIYXI'\n",
    "\n",
    "# Base URL for Google Places API\n",
    "url = 'https://maps.googleapis.com/maps/api/place/textsearch/json'\n",
    "\n",
    "# Load the postcode data (Assuming the file is correctly loaded into a DataFrame)\n",
    "postcodes_sdf = spark.read.parquet('../data/postcodes/postcodes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the current directory , we create separate files for our variables\n",
    "output_relative_dir = '../data/raw_variables/'\n",
    "variables = ['Hospitals & Clinics', 'Schools', 'Groceries']\n",
    "\n",
    "# check if it exists as it makedir will raise an error if it does exist\n",
    "if not os.path.exists(output_relative_dir):\n",
    "    os.makedirs(output_relative_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['locality', 'state', 'long', 'lat']\n",
    "postcodes_sdf = postcodes_sdf.drop(*columns)\n",
    "postcodes_sdf = postcodes_sdf.dropDuplicates()\n",
    "postcodes_sdf = postcodes_sdf.orderBy('postcode')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define schema for the Spark DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"Postcode\", StringType(), True),\n",
    "    StructField(\"Rating\", StringType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for testing purposes\n",
    "postcodes_sdf2 = postcodes_sdf.filter(postcodes_sdf['postcode'] < 3004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(postcodes_sdf) -> dict:\n",
    "    \"\"\"function that splits up postcodes into chunks of 50 so that if we are kicked halfway during scraping we don't lose too much progress\n",
    "    \"\"\"\n",
    "    chunk_dict = {}\n",
    "    i = 3000\n",
    "    j = 3050\n",
    "    \n",
    "    while i < 3997:\n",
    "        \n",
    "        temp = postcodes_sdf.filter((postcodes_sdf['postcode'] >= i) & (postcodes_sdf['postcode'] < j))\n",
    "\n",
    "        chunk_dict[f'chunk_{i}'] = temp\n",
    "        j += 50\n",
    "        i += 50\n",
    "\n",
    "    return chunk_dict\n",
    "\n",
    "chunk_dict = get_chunks(postcodes_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Scraping task 1: schools\n",
    "# Iterate through all variables and initialize a temporary dataframe\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "def variables_scrape(chunk, file_suffix):\n",
    "    variables = ['Hospitals & Clinics', 'Schools', 'Groceries']\n",
    "    schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"Postcode\", StringType(), True),\n",
    "    StructField(\"Rating\", StringType(), True),])\n",
    "    \n",
    "    variable_metadata = spark.createDataFrame([], schema)\n",
    "\n",
    "    postcodes_sdf.filter(postcodes_sdf['postcode'] <= 3000 + 250)\n",
    "    \n",
    "    for variable in variables:\n",
    "        # Loop through each row in the dataframe\n",
    "        for row in chunk.collect():\n",
    "            postcode = row['postcode']\n",
    "                \n",
    "            print(f'searching for {variable} in {postcode}')\n",
    "            # Define the search query using postcode\n",
    "            params = {\n",
    "                'query': f'{variable} in {postcode}, Victoria, Australia',\n",
    "                'key': API_KEY,\n",
    "                'type': {variable},\n",
    "                'region': 'AU'\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "                \n",
    "            # Check if the response was successful\n",
    "            if response.status_code == 200:\n",
    "                print(response.json())\n",
    "                results = response.json().get('results', [])\n",
    "                print(results)\n",
    "                    \n",
    "                # Write each place's details to the CSV file\n",
    "                for place in results:\n",
    "                    print(place)\n",
    "                    address = place.get('formatted_address')\n",
    "                    status = place.get('business_status')\n",
    "                    \n",
    "                    if (f'{postcode}' in address) & (status == 'OPERATIONAL'):\n",
    "                        print('match found')\n",
    "                        name = place.get('name')\n",
    "                        rating = place.get('rating', 'N/A')\n",
    "                        row = [(name, address, postcode, rating)]\n",
    "                        row_df = spark.createDataFrame(row, schema)\n",
    "                        variable_metadata = variable_metadata.union(row_df)\n",
    "                    \n",
    "                # Introduce a short delay to avoid hitting rate limits of the API\n",
    "                time.sleep(1)  # 1-second delay between requests\n",
    "            else:\n",
    "                print(f\"{variable}: Error fetching data for postcode {postcode}: {response.status_code}, {response.text}\")\n",
    "            print(f'searching for {variable} in {postcode}')\n",
    "\n",
    "        try: \n",
    "            variable_metadata.write.mode(\"overwrite\").parquet(f'../data/raw_variables/{variable}/{variable}_{file_suffix}.parquet')\n",
    "            print(f\"Data successfully written for {variable}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occured: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chunk(starting_chunk: int) -> None:\n",
    "    \"\"\"Function that scrapes domain.com.au in chunks of 25 postcodes 7 times (split amongst group members)\n",
    "    \n",
    "    Parameters:\n",
    "    starting_chunk - starting chunk number that we want\n",
    "\n",
    "    Return:\n",
    "    None \n",
    "    \"\"\"\n",
    "    i = starting_chunk\n",
    "    \n",
    "    while i < starting_chunk + 200: \n",
    "        variables_scrape(chunk_dict[f\"chunk_{i}\"], i) #i.split(\"_\")[1])\n",
    "        i += 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching for Hospitals & Clinics in 3150\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3150\n",
      "searching for Hospitals & Clinics in 3151\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3151\n",
      "searching for Hospitals & Clinics in 3152\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3152\n",
      "searching for Hospitals & Clinics in 3153\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3153\n",
      "searching for Hospitals & Clinics in 3154\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3154\n",
      "searching for Hospitals & Clinics in 3155\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3155\n",
      "searching for Hospitals & Clinics in 3156\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3156\n",
      "searching for Hospitals & Clinics in 3158\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3158\n",
      "searching for Hospitals & Clinics in 3159\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3159\n",
      "searching for Hospitals & Clinics in 3160\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3160\n",
      "searching for Hospitals & Clinics in 3161\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3161\n",
      "searching for Hospitals & Clinics in 3162\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3162\n",
      "searching for Hospitals & Clinics in 3163\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3163\n",
      "searching for Hospitals & Clinics in 3164\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3164\n",
      "searching for Hospitals & Clinics in 3165\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3165\n",
      "searching for Hospitals & Clinics in 3166\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3166\n",
      "searching for Hospitals & Clinics in 3167\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3167\n",
      "searching for Hospitals & Clinics in 3168\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3168\n",
      "searching for Hospitals & Clinics in 3169\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3169\n",
      "searching for Hospitals & Clinics in 3170\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3170\n",
      "searching for Hospitals & Clinics in 3171\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3171\n",
      "searching for Hospitals & Clinics in 3172\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3172\n",
      "searching for Hospitals & Clinics in 3173\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3173\n",
      "searching for Hospitals & Clinics in 3174\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3174\n",
      "searching for Hospitals & Clinics in 3175\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3175\n",
      "searching for Hospitals & Clinics in 3176\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3176\n",
      "searching for Hospitals & Clinics in 3177\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3177\n",
      "searching for Hospitals & Clinics in 3178\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3178\n",
      "searching for Hospitals & Clinics in 3179\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3179\n",
      "searching for Hospitals & Clinics in 3180\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3180\n",
      "searching for Hospitals & Clinics in 3181\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3181\n",
      "searching for Hospitals & Clinics in 3182\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3182\n",
      "searching for Hospitals & Clinics in 3183\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3183\n",
      "searching for Hospitals & Clinics in 3184\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n",
      "searching for Hospitals & Clinics in 3184\n",
      "searching for Hospitals & Clinics in 3185\n",
      "{'error_message': 'You must enable Billing on the Google Cloud Project at https://console.cloud.google.com/project/_/billing/enable Learn more at https://developers.google.com/maps/gmp-get-started', 'html_attributions': [], 'results': [], 'status': 'REQUEST_DENIED'}\n",
      "[]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Davyn \u001b[39;00m\n\u001b[1;32m      2\u001b[0m starting_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3150\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrun_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstarting_chunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m, in \u001b[0;36mrun_chunk\u001b[0;34m(starting_chunk)\u001b[0m\n\u001b[1;32m     10\u001b[0m i \u001b[38;5;241m=\u001b[39m starting_chunk\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m starting_chunk \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m200\u001b[39m: \n\u001b[0;32m---> 13\u001b[0m     \u001b[43mvariables_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#i.split(\"_\")[1])\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n",
      "Cell \u001b[0;32mIn[12], line 54\u001b[0m, in \u001b[0;36mvariables_scrape\u001b[0;34m(chunk, file_suffix)\u001b[0m\n\u001b[1;32m     51\u001b[0m             variable_metadata \u001b[38;5;241m=\u001b[39m variable_metadata\u001b[38;5;241m.\u001b[39munion(row_df)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# Introduce a short delay to avoid hitting rate limits of the API\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 1-second delay between requests\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Error fetching data for postcode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpostcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Davyn \n",
    "starting_chunk = 3150\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Arpan\n",
    "starting_chunk = 3000 + 200\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rachel\n",
    "starting_chunk = 3000 + 400\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nathan\n",
    "starting_chunk = 3000 + 600\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pris\n",
    "starting_chunk = 3000 + 800\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf = spark.read.parquet('../data/raw_variables/Groceries/*')\n",
    "# Create new parquet of raw data\n",
    "sdf \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('../data/scraped/groceries_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf = spark.read.parquet('../data/raw_variables/Hospitals & Clinics/*')\n",
    "# Create new parquet of raw data\n",
    "sdf \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('../data/scraped/Hospitals_&_Clinics_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf = spark.read.parquet('../data/raw_variables/Schools/*')\n",
    "# Create new parquet of raw data\n",
    "sdf \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('../data/scraped/Schools_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1369)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = spark.read.parquet('../data/scraped/Schools_data.parquet')\n",
    "len(sdf.columns), sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/home/rachel/Assignment 2/project-2-group-real-estate-industry-project-22/data/scraped/Groceries_data.parquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sdf \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/scraped/Groceries_data.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mlen\u001b[39m(sdf\u001b[38;5;241m.\u001b[39mcolumns), sdf\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    542\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/home/rachel/Assignment 2/project-2-group-real-estate-industry-project-22/data/scraped/Groceries_data.parquet."
     ]
    }
   ],
   "source": [
    "sdf = spark.read.parquet('../data/scraped/Groceries_data.parquet')\n",
    "len(sdf.columns), sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.parquet('../data/scraped/Hospitals_&_Clinics_data.parquet')\n",
    "len(sdf.columns), sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_sdf = spark.read.parquet('../data/scraped/Schools_data.parquet')\n",
    "# Group by Postcode and count the number of schools\n",
    "schools_per_postcode = school_sdf.groupBy('Postcode').agg(F.count('Name').alias('School_Count'))\n",
    "schools_per_postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groceries_sdf = spark.read.parquet('../data/scraped/groceries_data.parquet')\n",
    "# Group by Postcode and count the number of groceriess\n",
    "groceries_per_postcode = groceries_sdf.groupBy('Postcode').agg(F.count('Name').alias('groceries_Count'))\n",
    "groceries_per_postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_sdf = spark.read.parquet('../data/scraped/Hospitals_&_Clinics_data.parquet')\n",
    "# Group by Postcode and count the number of hcs\n",
    "hc_per_postcode = hc_sdf.groupBy('Postcode').agg(F.count('Name').alias('Number of Healcare'))\n",
    "hc_per_postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a join on Postcode column to combine all three DataFrames\n",
    "combined_df = schools_per_postcode \\\n",
    "    .join(groceries_per_postcode, on='Postcode', how='outer') \\\n",
    "    .join(hc_per_postcode, on='Postcode', how='outer')\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the UDF to get latitude and longitude from Google API\n",
    "def get_geolocation(postcode):\n",
    "    url = f\"https://maps.googleapis.com/maps/api/geocode/json?address={postcode},Victoria,Australia&key={API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        if result['results']:\n",
    "            location = result['results'][0]['geometry']['location']\n",
    "            return location['lat'], location['lng']\n",
    "    return None, None\n",
    "\n",
    "# Split the function into two UDFs: one for latitude, one for longitude\n",
    "def get_latitude(postcode):\n",
    "    lat, lng = get_geolocation(postcode)\n",
    "    return lat\n",
    "\n",
    "def get_longitude(postcode):\n",
    "    lat, lng = get_geolocation(postcode)\n",
    "    return lng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register UDFs with PySpark\n",
    "get_latitude_udf = udf(get_latitude, FloatType())\n",
    "get_longitude_udf = udf(get_longitude, FloatType())\n",
    "\n",
    "# Assuming you have a DataFrame `combined_update_sdf` with a 'Postcode' column\n",
    "# For example:\n",
    "combined_update_sdf = combined_df\n",
    "\n",
    "# Add latitude and longitude columns to your DataFrame\n",
    "combined_update_sdf = combined_update_sdf.withColumn('Latitude', get_latitude_udf(combined_update_sdf['Postcode']))\n",
    "combined_update_sdf = combined_update_sdf.withColumn('Longitude', get_longitude_udf(combined_update_sdf['Postcode']))\n",
    "\n",
    "# Show the updated DataFrame with geolocation data\n",
    "combined_update_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_update_sdf \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('../data/scraped/combined_data_with_geolocation.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get suburb/locality using Google API\n",
    "def get_suburb_name(postcode):\n",
    "    url = f\"https://maps.googleapis.com/maps/api/geocode/json?address={postcode},Victoria,Australia&key={API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        if result['results']:\n",
    "            for component in result['results'][0]['address_components']:\n",
    "                if 'locality' in component['types']:  # Extract the locality (suburb)\n",
    "                    return component['long_name']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the UDF with PySpark\n",
    "get_suburb_name_udf = udf(get_suburb_name, StringType())\n",
    "\n",
    "# Assuming you have a DataFrame 'combined_df' with 'Postcode' column\n",
    "# Apply the UDF to add suburb names to your DataFrame\n",
    "combined_sdf_with_names = combined_update_sdf.withColumn('Suburb', get_suburb_name_udf(combined_df['Postcode']))\n",
    "\n",
    "# Show the DataFrame with suburb names\n",
    "combined_sdf_with_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sdf_with_names \\\n",
    "                .coalesce(1) \\\n",
    "                .write \\\n",
    "                .mode('overwrite') \\\n",
    "                .parquet('../data/scraped/combined_data_with_geolocation.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sdf = spark.read.parquet('../data/scraped/combined_data_with_geolocation.parquet')\n",
    "combined_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sdf = spark.read.parquet('../data/curated/merged_df.parquet')\n",
    "merged_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveability_sdf = merged_sdf \\\n",
    "            .join(combined_sdf, on='Postcode', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Haversine formula using PySpark\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Convert degrees to radians\n",
    "    lat1 = F.radians(lat1)\n",
    "    lon1 = F.radians(lon1)\n",
    "    lat2 = F.radians(lat2)\n",
    "    lon2 = F.radians(lon2)\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = F.sin(dlat / 2) ** 2 + F.cos(lat1) * F.cos(lat2) * F.sin(dlon / 2) ** 2\n",
    "    c = 2 * F.atan2(F.sqrt(a), F.sqrt(1 - a))\n",
    "    \n",
    "    # Radius of Earth in kilometers\n",
    "    r = 6371.0\n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Melbourne CBD's coordinates\n",
    "melbourne_lat, melbourne_lon = -37.8136, 144.9631"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Haversine formula to the Spark DataFrame\n",
    "liveability_sdf = liveability_sdf.withColumn(\n",
    "    \"distance_to_melbourne_km\",\n",
    "    haversine(F.col(\"Latitude\"), F.col(\"Longitude\"), F.lit(melbourne_lat), F.lit(melbourne_lon))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add condition to replace distances < 1 km with 0\n",
    "liveability_sdf = liveability_sdf.withColumn(\n",
    "    \"distance_to_melbourne_km\",\n",
    "    F.when(F.col(\"distance_to_melbourne_km\") < 1, 0).otherwise(F.col(\"distance_to_melbourne_km\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate school per capita and add a new column\n",
    "liveability_sdf = liveability_sdf.withColumn(\n",
    "    \"school_per_capita\", \n",
    "    F.col(\"School_Count\") / F.col(\"total population - 2021\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveability_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveability_sdf \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('../data/scraped/liveability_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
