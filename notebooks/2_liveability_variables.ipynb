{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/03 22:05:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Liveability\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config('spark.driver.memory', '4g')\n",
    "    .config('spark.executor.memory', '2g')\n",
    "    .getOrCreate()\n",
    ")\n",
    "df = spark.read.parquet(\"../data/postcodes/postcodes.parquet\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Google Places API key\n",
    "API_KEY = 'AIzaSyDKBch72s8hyaVK4GsnrOhA5AnWT4IIYXI'\n",
    "\n",
    "# Base URL for Google Places API\n",
    "url = 'https://maps.googleapis.com/maps/api/place/textsearch/json'\n",
    "\n",
    "# Load the postcode data (Assuming the file is correctly loaded into a DataFrame)\n",
    "postcodes_sdf = spark.read.parquet('../data/postcodes/postcodes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "\n",
    "# from the current directory , we create separate files for our variables\n",
    "output_relative_dir = '../data/raw_variables/'\n",
    "variables = ['Hospitals & Clinics', 'Schools', 'Groceries']\n",
    "\n",
    "# check if it exists as it makedir will raise an error if it does exist\n",
    "if not os.path.exists(output_relative_dir):\n",
    "    os.makedirs(output_relative_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['locality', 'state', 'long', 'lat']\n",
    "postcodes_sdf = postcodes_sdf.drop(*columns)\n",
    "postcodes_sdf = postcodes_sdf.dropDuplicates()\n",
    "postcodes_sdf = postcodes_sdf.orderBy('postcode')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Define schema for the Spark DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"Postcode\", StringType(), True),\n",
    "    StructField(\"Rating\", StringType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for testing purposes\n",
    "postcodes_sdf2 = postcodes_sdf.filter(postcodes_sdf['postcode'] < 3004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(postcodes_sdf) -> dict:\n",
    "    \"\"\"function that splits up postcodes into chunks of 50 so that if we are kicked halfway during scraping we don't lose too much progress\n",
    "    \"\"\"\n",
    "    chunk_dict = {}\n",
    "    i = 3000\n",
    "    j = 3050\n",
    "    \n",
    "    while i < 3997:\n",
    "        \n",
    "        temp = postcodes_sdf.filter((postcodes_sdf['postcode'] >= i) & (postcodes_sdf['postcode'] < j))\n",
    "\n",
    "        chunk_dict[f'chunk_{i}'] = temp\n",
    "        j += 50\n",
    "        i += 50\n",
    "\n",
    "    return chunk_dict\n",
    "\n",
    "chunk_dict = get_chunks(postcodes_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Scraping task 1: schools\n",
    "# Iterate through all variables and initialize a temporary dataframe\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "def variables_scrape(chunk, file_suffix):\n",
    "    variables = ['Hospitals & Clinics', 'Schools', 'Groceries']\n",
    "    schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"Postcode\", StringType(), True),\n",
    "    StructField(\"Rating\", StringType(), True),])\n",
    "    \n",
    "    variable_metadata = spark.createDataFrame([], schema)\n",
    "\n",
    "    postcodes_sdf.filter(postcodes_sdf['postcode'] <= 3000 + 250)\n",
    "    \n",
    "    for variable in variables:\n",
    "        # Loop through each row in the dataframe\n",
    "        for row in chunk.collect():\n",
    "            postcode = row['postcode']\n",
    "                \n",
    "            print(f'searching for {variable} in {postcode}')\n",
    "            # Define the search query using postcode\n",
    "            params = {\n",
    "                'query': f'{variable} in {postcode}, Victoria, Australia',\n",
    "                'key': API_KEY,\n",
    "                'type': {variable},\n",
    "                'region': 'AU'\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "                \n",
    "            # Check if the response was successful\n",
    "            if response.status_code == 200:\n",
    "                print(response.json())\n",
    "                results = response.json().get('results', [])\n",
    "                print(results)\n",
    "                    \n",
    "                # Write each place's details to the CSV file\n",
    "                for place in results:\n",
    "                    print(place)\n",
    "                    address = place.get('formatted_address')\n",
    "                    status = place.get('business_status')\n",
    "                    \n",
    "                    if (f'{postcode}' in address) & (status == 'OPERATIONAL'):\n",
    "                        print('match found')\n",
    "                        name = place.get('name')\n",
    "                        rating = place.get('rating', 'N/A')\n",
    "                        row = [(name, address, postcode, rating)]\n",
    "                        row_df = spark.createDataFrame(row, schema)\n",
    "                        variable_metadata = variable_metadata.union(row_df)\n",
    "                    \n",
    "                # Introduce a short delay to avoid hitting rate limits of the API\n",
    "                time.sleep(1)  # 1-second delay between requests\n",
    "            else:\n",
    "                print(f\"{variable}: Error fetching data for postcode {postcode}: {response.status_code}, {response.text}\")\n",
    "            print(f'searching for {variable} in {postcode}')\n",
    "\n",
    "        try: \n",
    "            variable_metadata.write.mode(\"overwrite\").parquet(f'../data/raw_variables/{variable}/{variable}_{file_suffix}.parquet')\n",
    "            print(f\"Data successfully written for {variable}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occured: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chunk(starting_chunk: int) -> None:\n",
    "    \"\"\"Function that scrapes domain.com.au in chunks of 25 postcodes 7 times (split amongst group members)\n",
    "    \n",
    "    Parameters:\n",
    "    starting_chunk - starting chunk number that we want\n",
    "\n",
    "    Return:\n",
    "    None \n",
    "    \"\"\"\n",
    "    i = starting_chunk\n",
    "    \n",
    "    while i < starting_chunk + 200: \n",
    "        variables_scrape(chunk_dict[f\"chunk_{i}\"], i) #i.split(\"_\")[1])\n",
    "        i += 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Davyn \n",
    "starting_chunk = 3150\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Arpan\n",
    "starting_chunk = 3000 + 200\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rachel\n",
    "starting_chunk = 3000 + 400\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nathan\n",
    "starting_chunk = 3000 + 600\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pris\n",
    "starting_chunk = 3000 + 800\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Name</th><th>Address</th><th>Postcode</th><th>Rating</th></tr>\n",
       "<tr><td>Blackwood Special...</td><td>Special School Ou...</td><td>3458</td><td>4.5</td></tr>\n",
       "<tr><td>BreastScreen Mary...</td><td>Maryborough Distr...</td><td>3465</td><td>0</td></tr>\n",
       "<tr><td>Trentham District...</td><td>Trentham District...</td><td>3458</td><td>5</td></tr>\n",
       "<tr><td>FLO Program, Etty...</td><td>35 Etty St, Castl...</td><td>3450</td><td>0</td></tr>\n",
       "<tr><td>Olinda Primary Sc...</td><td>Olinda State Scho...</td><td>3494</td><td>3.9</td></tr>\n",
       "<tr><td>Dhelkaya Health -...</td><td>Castlemaine Healt...</td><td>3450</td><td>0</td></tr>\n",
       "<tr><td>Terra Australis D...</td><td>Lot 2 Railway Cre...</td><td>3460</td><td>4.6</td></tr>\n",
       "<tr><td>Dorevitch Pathology</td><td>Maryborough &amp; Dis...</td><td>3465</td><td>4.7</td></tr>\n",
       "<tr><td>Roseberry House E...</td><td>123 Inkerman St, ...</td><td>3465</td><td>5</td></tr>\n",
       "<tr><td>Milla Dance Studi...</td><td>57/31 Lyttleton S...</td><td>3450</td><td>5</td></tr>\n",
       "<tr><td>Etty street Campu...</td><td>35 Etty St, Castl...</td><td>3450</td><td>0</td></tr>\n",
       "<tr><td>Your Health Place...</td><td>63 Elizabeth St, ...</td><td>3450</td><td>4.1</td></tr>\n",
       "<tr><td>Villa Sophia - Me...</td><td>74 Patterson St, ...</td><td>3460</td><td>0</td></tr>\n",
       "<tr><td>Maryborough Distr...</td><td>75/87 Clarendon S...</td><td>3465</td><td>3.7</td></tr>\n",
       "<tr><td>Castlemaine Media...</td><td>Level 2, Room 6, ...</td><td>3450</td><td>0</td></tr>\n",
       "<tr><td>Nangiloc Colignan...</td><td>2612 Kulkyne Way,...</td><td>3494</td><td>5</td></tr>\n",
       "<tr><td>Maryborough Distr...</td><td>20 Havelock St, D...</td><td>3472</td><td>5</td></tr>\n",
       "<tr><td>Castlemaine Stein...</td><td>11 Rilens Rd, Muc...</td><td>3451</td><td>5</td></tr>\n",
       "<tr><td>Long Paddock Chee...</td><td>9 Walker St, Cast...</td><td>3450</td><td>4.9</td></tr>\n",
       "<tr><td>Castlemaine YMCA ...</td><td>129 Main Rd, Camp...</td><td>3451</td><td>0</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+--------------------+--------+------+\n",
       "|                Name|             Address|Postcode|Rating|\n",
       "+--------------------+--------------------+--------+------+\n",
       "|Blackwood Special...|Special School Ou...|    3458|   4.5|\n",
       "|BreastScreen Mary...|Maryborough Distr...|    3465|     0|\n",
       "|Trentham District...|Trentham District...|    3458|     5|\n",
       "|FLO Program, Etty...|35 Etty St, Castl...|    3450|     0|\n",
       "|Olinda Primary Sc...|Olinda State Scho...|    3494|   3.9|\n",
       "|Dhelkaya Health -...|Castlemaine Healt...|    3450|     0|\n",
       "|Terra Australis D...|Lot 2 Railway Cre...|    3460|   4.6|\n",
       "| Dorevitch Pathology|Maryborough & Dis...|    3465|   4.7|\n",
       "|Roseberry House E...|123 Inkerman St, ...|    3465|     5|\n",
       "|Milla Dance Studi...|57/31 Lyttleton S...|    3450|     5|\n",
       "|Etty street Campu...|35 Etty St, Castl...|    3450|     0|\n",
       "|Your Health Place...|63 Elizabeth St, ...|    3450|   4.1|\n",
       "|Villa Sophia - Me...|74 Patterson St, ...|    3460|     0|\n",
       "|Maryborough Distr...|75/87 Clarendon S...|    3465|   3.7|\n",
       "|Castlemaine Media...|Level 2, Room 6, ...|    3450|     0|\n",
       "|Nangiloc Colignan...|2612 Kulkyne Way,...|    3494|     5|\n",
       "|Maryborough Distr...|20 Havelock St, D...|    3472|     5|\n",
       "|Castlemaine Stein...|11 Rilens Rd, Muc...|    3451|     5|\n",
       "|Long Paddock Chee...|9 Walker St, Cast...|    3450|   4.9|\n",
       "|Castlemaine YMCA ...|129 Main Rd, Camp...|    3451|     0|\n",
       "+--------------------+--------------------+--------+------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_sdf = spark.read.parquet('../data/raw_variables/Schools/Schools_3450.parquet')\n",
    "testing_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.parquet('../data/raw_variables/Groceries')\n",
    "\n",
    "# Create new parquet of raw data\n",
    "sdf \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('../data/scrapped/groceries_data.parquet')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.parquet('../data/raw_variables/Hospitals & Clinics')\n",
    "\n",
    "# Create new parquet of raw data\n",
    "sdf \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('../data/scrapped/Hospitals_&_Clinics_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.parquet('../data/raw_variables/Groceries')\n",
    "# Create new parquet of raw data\n",
    "sdf \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('../data/scrapped/groceries_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO NOT RUN FROM HERE ONWARDS #########\n",
    "## DO NOT RUN\n",
    "# Iterate through all variables and initialize a temporary dataframe\n",
    "for variable in variables :\n",
    "    variable_metadata = spark.createDataFrame([], schema)\n",
    "\n",
    "    # Loop through each row in the dataframe\n",
    "    for row in postcodes_sdf.collect():\n",
    "        postcode = row['postcode']\n",
    "        \n",
    "        print(f'searching for {variable} in {postcode}')\n",
    "        # Define the search query using postcode\n",
    "        params = {\n",
    "            'query': f'{variable} in {postcode}, Victoria, Australia',\n",
    "            'key': API_KEY,\n",
    "            'type': {variable},\n",
    "            'region': 'AU'\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        \n",
    "        # Check if the response was successful\n",
    "        if response.status_code == 200:\n",
    "            results = response.json().get('results', [])\n",
    "            \n",
    "            # Write each place's details to the CSV file\n",
    "            for place in results:\n",
    "                name = place.get('name')\n",
    "                address = place.get('formatted_address')\n",
    "                rating = place.get('rating', 'N/A')\n",
    "                row = [(name, address, postcode, rating)]\n",
    "                row_df = spark.createDataFrame(row, schema)\n",
    "                variable_metadata = variable_metadata.union(row_df)\n",
    "            \n",
    "            # Introduce a short delay to avoid hitting rate limits of the API\n",
    "            time.sleep(1)  # 1-second delay between requests\n",
    "        else:\n",
    "            print(f\"{variable}: Error fetching data for postcode {postcode}: {response.status_code}, {response.text}\")\n",
    "        print(f'searching for {variable} in {postcode}')\n",
    "\n",
    "    try:\n",
    "        variable_metadata.write.mode(\"overwrite\").parquet(f'../data/landing/{variable}.parquet')\n",
    "        print(f\"Data successfully written for {variable}\")\n",
    "    except Exception as e:\n",
    "       print(f\"An error occured: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('../data/landing/supermarkets_by_postcode.csv')\n",
    "\n",
    "# Filter the rows where the Postcode in the Address matches exactly the Postcode column\n",
    "df_filtered = df[df.apply(lambda row: str(row['Postcode']) in row['Address'], axis=1)]\n",
    "\n",
    "# Drop duplicates from the filtered DataFrame\n",
    "df_filtered_unique = df_filtered.drop_duplicates()\n",
    "\n",
    "# Reset the index of the filtered DataFrame without duplicates\n",
    "df_filtered_unique_reset = df_filtered_unique.reset_index(drop=True)\n",
    "\n",
    "df_filtered_unique_reset.tail(20)\n",
    "\n",
    "df_filtered_unique_reset.to_csv('../data/raw/supermarket.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
