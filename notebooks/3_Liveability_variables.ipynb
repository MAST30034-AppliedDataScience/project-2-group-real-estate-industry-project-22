{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liveability Variables\n",
    "\n",
    "**Jupyter notebook that scrapes grocery stores, healthcare services and schools in each suburbs with outputs the information collected into a single parquet files. The parquet also include distance of each suburbs to Melbourne CBD**\n",
    "\n",
    "Below are some functions that will help with scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we imported necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, FloatType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then initialised a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/10/06 04:40:05 WARN Utils: Your hostname, DESKTOP-Q5SP5SI resolves to a loopback address: 127.0.1.1; using 172.20.36.110 instead (on interface eth0)\n",
      "24/10/06 04:40:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/06 04:40:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/10/06 04:40:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/10/06 04:40:14 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Liveability\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config('spark.driver.memory', '4g')\n",
    "    .config('spark.executor.memory', '2g')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API key and base URL for Google Places was added. Additionally, data containing postcode information was read using Apache Spark. This postcode dataframe would allow us to iteratively scrape information on available schoosl, groceries and healthcare for every postcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Google Places API key\n",
    "API_KEY = 'INPUT YOUR API KEY'\n",
    "\n",
    "# Base URL for Google Places API\n",
    "url = 'https://maps.googleapis.com/maps/api/place/textsearch/json'\n",
    "\n",
    "# Load the postcode data (Assuming the file is correctly loaded into a DataFrame)\n",
    "postcodes_sdf = spark.read.parquet('../data/postcodes/postcodes.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then created necessary data directories to store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the current directory , we create separate files for our variables\n",
    "output_relative_dir = '../data/raw_variables/'\n",
    "variables = ['Hospitals & Clinics', 'Schools', 'Groceries']\n",
    "\n",
    "# check if it exists as it makedir will raise an error if it does exist\n",
    "if not os.path.exists(output_relative_dir):\n",
    "    os.makedirs(output_relative_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As only the postcode and suburb names were required for scraping data, unnecessary columns of locality state , longitude and latitude were removed. Duplicate entries were also removed for a more seamless scraping process. Additionally, the postcodes were ordered in an ascnending  order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to remove\n",
    "columns = ['locality', 'state', 'long', 'lat']\n",
    "postcodes_sdf = postcodes_sdf.drop(*columns)\n",
    "postcodes_sdf = postcodes_sdf.dropDuplicates()\n",
    "postcodes_sdf = postcodes_sdf.orderBy('postcode')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then created a schema for information of each metric that was required to be scraped. This included the :\n",
    "- name\n",
    "- address\n",
    "- postcode\n",
    "- rating\n",
    "\n",
    "for grocery stores, healthcare and educational hubs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for the Spark DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"Postcode\", StringType(), True),\n",
    "    StructField(\"Rating\", StringType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid Out of Memory errors from our limited RAM, additional codes were written to scrape the postcodes in chunks of 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(postcodes_sdf) -> dict:\n",
    "    \"\"\"function that splits up postcodes into chunks of 50 so that if we are kicked halfway during scraping we don't lose too much progress\n",
    "    \"\"\"\n",
    "    chunk_dict = {}\n",
    "    i = 3000\n",
    "    j = 3050\n",
    "    \n",
    "    # iterate each postcode until the final postcode o  399&\n",
    "    while i < 3997:\n",
    "        \n",
    "        # filter for all postcodes within a specified range\n",
    "        temp = postcodes_sdf.filter((postcodes_sdf['postcode'] >= i) & (postcodes_sdf['postcode'] < j))\n",
    "\n",
    "        # append the postcodes into a dictionary with their chunk name\n",
    "        chunk_dict[f'chunk_{i}'] = temp\n",
    "        j += 50\n",
    "        i += 50\n",
    "\n",
    "    return chunk_dict\n",
    "\n",
    "chunk_dict = get_chunks(postcodes_sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main problems we encountered when scraping for these buildings was having buildings (grocery stores/schools or hospitals) in nearby suburbs show up as a search. Hence, additional matching steps had to be taken to ensure that the postcode of these buildings match the postcode of the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Scraping task 1: schools\n",
    "# Iterate through all variables and initialize a temporary dataframe\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "def variables_scrape(chunk, file_suffix):\n",
    "    \"\"\"function that obtains the chunk containing 50 postcodes to scrape. It outputs the the data into a parquet file namesd after the file suffix specified.\n",
    "    \"\"\"\n",
    "\n",
    "    # A schema containing the necessary variables was set up \n",
    "    variables = ['Hospitals & Clinics', 'Schools', 'Groceries']\n",
    "    schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"Postcode\", StringType(), True),\n",
    "    StructField(\"Rating\", StringType(), True),])\n",
    "    \n",
    "    # Initialize an empty dataframe \n",
    "    variable_metadata = spark.createDataFrame([], schema)\n",
    "\n",
    "    # Iterate all the variables\n",
    "    for variable in variables:\n",
    "\n",
    "        # Loop through each row in the dataframe\n",
    "        for row in chunk.collect():\n",
    "            postcode = row['postcode']\n",
    "                \n",
    "            print(f'searching for {variable} in {postcode}')\n",
    "            # Define the search query using postcode\n",
    "            params = {\n",
    "                'query': f'{variable} in {postcode}, Victoria, Australia',\n",
    "                'key': API_KEY,\n",
    "                'type': {variable},\n",
    "                'region': 'AU'\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "                \n",
    "            # Check if the response was successful\n",
    "            if response.status_code == 200:\n",
    "                print(response.json())\n",
    "                results = response.json().get('results', [])\n",
    "                print(results)\n",
    "                    \n",
    "                # Write each place's details to the CSV file\n",
    "                for place in results:\n",
    "                    print(place)\n",
    "                    address = place.get('formatted_address')\n",
    "                    status = place.get('business_status')\n",
    "                    \n",
    "                    # If there is a particular building available in a postcode, append the building into the matching postcode\n",
    "                    if (f'{postcode}' in address) & (status == 'OPERATIONAL'):\n",
    "                        print('match found')\n",
    "                        name = place.get('name')\n",
    "                        rating = place.get('rating', 'N/A')\n",
    "                        row = [(name, address, postcode, rating)]\n",
    "                        row_df = spark.createDataFrame(row, schema)\n",
    "                        variable_metadata = variable_metadata.union(row_df)\n",
    "                    \n",
    "                # Introduce a short delay to avoid hitting rate limits of the API\n",
    "                time.sleep(1)  # 1-second delay between requests\n",
    "            else:\n",
    "                print(f\"{variable}: Error fetching data for postcode {postcode}: {response.status_code}, {response.text}\")\n",
    "            print(f'searching for {variable} in {postcode}')\n",
    "\n",
    "        # Output all the data that was scraped. \n",
    "        try: \n",
    "            variable_metadata.write.mode(\"overwrite\").parquet(f'../data/raw_variables/{variable}/{variable}_{file_suffix}.parquet')\n",
    "            print(f\"Data successfully written for {variable}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occured: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to split up the scraping task, we divided the all task by having each person scrape 4 chunks (50 postcodes per chunk/ 200 postcodes in total). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chunk(starting_chunk: int) -> None:\n",
    "    \"\"\"Function that scrapes domain.com.au in chunks of 25 postcodes 7 times (split amongst group members)\n",
    "    \n",
    "    Parameters:\n",
    "    starting_chunk - starting chunk number that we want\n",
    "\n",
    "    Return:\n",
    "    None \n",
    "    \"\"\"\n",
    "    i = starting_chunk\n",
    "    \n",
    "    while i < starting_chunk + 200: \n",
    "        variables_scrape(chunk_dict[f\"chunk_{i}\"], i) #i.split(\"_\")[1])\n",
    "        i += 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Down below we split up the scraping process between team members to scrape more efficiently and minimise loss of time due to hardware limitations **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Davyn \n",
    "starting_chunk = 3150\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Arpan\n",
    "starting_chunk = 3000 + 200\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rachel\n",
    "starting_chunk = 3000 + 400\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nathan\n",
    "starting_chunk = 3000 + 600\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pris\n",
    "starting_chunk = 3000 + 800\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining Data Frame for each categories (i.e. groceries store, healthcare services and schools)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data for each of the variable was comalesced into one single parquet file. This resulted in 3 separate parquet files for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf = spark.read.parquet('../data/raw_variables/Groceries/*')\n",
    "# Create new parquet of raw data\n",
    "sdf \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('../data/scraped/groceries_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf = spark.read.parquet('../data/raw_variables/Hospitals & Clinics/*')\n",
    "# Create new parquet of raw data\n",
    "sdf \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('../data/scraped/Hospitals_&_Clinics_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/03 22:30:06 WARN TaskSetManager: Stage 84 contains a task of very large size (1457 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf = spark.read.parquet('../data/raw_variables/Schools/*')\n",
    "# Create new parquet of raw data\n",
    "sdf \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('../data/scraped/Schools_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count the number of buildings based on their postcode**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of available schools, grocery stores and healthcare hubs are calculated for each suburb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_sdf = spark.read.parquet('../data/scraped/Schools_data.parquet')\n",
    "# Group by Postcode and count the number of schools\n",
    "schools_per_postcode = school_sdf.groupBy('Postcode').agg(F.count('Name').alias('School_Count'))\n",
    "schools_per_postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groceries_sdf = spark.read.parquet('../data/scraped/groceries_data.parquet')\n",
    "# Group by Postcode and count the number of groceriess\n",
    "groceries_per_postcode = groceries_sdf.groupBy('Postcode').agg(F.count('Name').alias('groceries_Count'))\n",
    "groceries_per_postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_sdf = spark.read.parquet('../data/scraped/Hospitals_&_Clinics_data.parquet')\n",
    "# Group by Postcode and count the number of hcs\n",
    "hc_per_postcode = hc_sdf.groupBy('Postcode').agg(F.count('Name').alias('Number of Healcare'))\n",
    "hc_per_postcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining into a single file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of this was then joined into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a join on Postcode column to combine all three DataFrames\n",
    "combined_df = schools_per_postcode \\\n",
    "    .join(groceries_per_postcode, on='Postcode', how='outer') \\\n",
    "    .join(hc_per_postcode, on='Postcode', how='outer')\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Session to get longitude and latitude for each postcode**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latitude and longitude for each postcode was obtained to calculate their proximity to the city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the UDF to get latitude and longitude from Google API\n",
    "def get_geolocation(postcode):\n",
    "    url = f\"https://maps.googleapis.com/maps/api/geocode/json?address={postcode},Victoria,Australia&key={API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        if result['results']:\n",
    "            location = result['results'][0]['geometry']['location']\n",
    "            return location['lat'], location['lng']\n",
    "    return None, None\n",
    "\n",
    "# Split the function into two UDFs: one for latitude, one for longitude\n",
    "def get_latitude(postcode):\n",
    "    lat, lng = get_geolocation(postcode)\n",
    "    return lat\n",
    "\n",
    "def get_longitude(postcode):\n",
    "    lat, lng = get_geolocation(postcode)\n",
    "    return lng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register UDFs with PySpark\n",
    "get_latitude_udf = udf(get_latitude, FloatType())\n",
    "get_longitude_udf = udf(get_longitude, FloatType())\n",
    "\n",
    "# Assuming you have a DataFrame `combined_update_sdf` with a 'Postcode' column\n",
    "# For example:\n",
    "combined_update_sdf = combined_df\n",
    "\n",
    "# Add latitude and longitude columns to your DataFrame\n",
    "combined_update_sdf = combined_update_sdf.withColumn('Latitude', get_latitude_udf(combined_update_sdf['Postcode']))\n",
    "combined_update_sdf = combined_update_sdf.withColumn('Longitude', get_longitude_udf(combined_update_sdf['Postcode']))\n",
    "\n",
    "# Show the updated DataFrame with geolocation data\n",
    "combined_update_sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the suburb name from each postcode using Google Maps API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get suburb/locality using Google API\n",
    "def get_suburb_name(postcode):\n",
    "    url = f\"https://maps.googleapis.com/maps/api/geocode/json?address={postcode},Victoria,Australia&key={API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        if result['results']:\n",
    "            for component in result['results'][0]['address_components']:\n",
    "                if 'locality' in component['types']:  # Extract the locality (suburb)\n",
    "                    return component['long_name']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the UDF with PySpark\n",
    "get_suburb_name_udf = udf(get_suburb_name, StringType())\n",
    "\n",
    "# Assuming you have a DataFrame 'combined_df' with 'Postcode' column\n",
    "# Apply the UDF to add suburb names to your DataFrame\n",
    "combined_sdf_with_names = combined_update_sdf.withColumn('Suburb', get_suburb_name_udf(combined_df['Postcode']))\n",
    "\n",
    "# Show the DataFrame with suburb names\n",
    "combined_sdf_with_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform data merge merged dataframe with geolocation dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocation_sdf =combined_sdf_with_names\n",
    "merged_sdf = spark.read.parquet('../data/curated/merged_df.parquet')\n",
    "liveability_sdf = merged_sdf \\\n",
    "            .join(geolocation_sdf, on='Postcode', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform calculation distance to Melbourne CBD from each suburbs and remove distance to CBD when it less than 1KM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Haversine formula using PySpark\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Convert degrees to radians\n",
    "    lat1 = F.radians(lat1)\n",
    "    lon1 = F.radians(lon1)\n",
    "    lat2 = F.radians(lat2)\n",
    "    lon2 = F.radians(lon2)\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = F.sin(dlat / 2) ** 2 + F.cos(lat1) * F.cos(lat2) * F.sin(dlon / 2) ** 2\n",
    "    c = 2 * F.atan2(F.sqrt(a), F.sqrt(1 - a))\n",
    "    \n",
    "    # Radius of Earth in kilometers\n",
    "    r = 6371.0\n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Melbourne CBD's coordinates\n",
    "melbourne_lat, melbourne_lon = -37.8136, 144.9631"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Haversine formula to the Spark DataFrame\n",
    "liveability_sdf = liveability_sdf.withColumn(\n",
    "    \"distance_to_melbourne_km\",\n",
    "    haversine(F.col(\"Latitude\"), F.col(\"Longitude\"), F.lit(melbourne_lat), F.lit(melbourne_lon))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add condition to replace distances < 1 km with 0\n",
    "liveability_sdf = liveability_sdf.withColumn(\n",
    "    \"distance_to_melbourne_km\",\n",
    "    F.when(F.col(\"distance_to_melbourne_km\") < 1, 0).otherwise(F.col(\"distance_to_melbourne_km\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate school per capita and add a new column\n",
    "liveability_sdf = liveability_sdf.withColumn(\n",
    "    \"school_per_capita\", \n",
    "    F.col(\"School_Count\") / F.col(\"total population - 2021\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the final dataframe of liveability index to a parquet file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final dataframe into a parquet file\n",
    "liveability_sdf \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('../data/scraped/liveability_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
