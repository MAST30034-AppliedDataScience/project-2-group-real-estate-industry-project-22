{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/11 11:24:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "## METHOD 1: convert dictionary to spark dataframe and append to initialized sdf\n",
    "# built-in imports\n",
    "import re\n",
    "from json import dump, load\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "import pandas as pd  \n",
    "import os\n",
    "# Import Spark modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Domain Scraper\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#### create a spark data frame\n",
    "\n",
    "# constants\n",
    "BASE_URL = \"https://www.domain.com.au\"\n",
    "N_PAGES = range(1, 2)  # Update this to your liking\n",
    "\n",
    "#Scrape suburb from the address\n",
    "def extract_suburb(address: str) -> str:\n",
    "    \"\"\"Extract the suburb name from the property address.\"\"\"\n",
    "    match = re.search(r'(?<=, )\\w+', address)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def start_scrape() -> None:\n",
    "    \"\"\" Function that scrapes https://www.domain.com.au and outputs the data into a json file\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    schema = StructType([\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"cost_text\", StringType(), True),\n",
    "    StructField(\"rooms\", StringType(), True),\n",
    "    StructField(\"desc\", StringType(), True),\n",
    "    StructField(\"parking\", StringType(), True),\n",
    "    StructField(\"street\", StringType(), True),\n",
    "    StructField(\"suburb\", StringType(), True),\n",
    "    StructField(\"postcode\", StringType(), True),\n",
    "    StructField(\"propertyType\", StringType(), True),\n",
    "    StructField(\"school\", StringType(), True),\n",
    "    StructField(\"features\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "    # begin code\n",
    "    url_links = []\n",
    "    property_metadata = defaultdict(dict)\n",
    "    sdf = spark.createDataFrame([],schema)\n",
    "    \n",
    "\n",
    "    # generate list of urls to visit\n",
    "    for page in N_PAGES:\n",
    "        url = BASE_URL + f\"/rent/?excludedeposittaken=1&state=vic&page={page}\"\n",
    "        print(f\"Visiting {url}\")\n",
    "        bs_object = BeautifulSoup(urlopen(Request(url, headers={'User-Agent': \"PostmanRuntime/7.6.0\"})), \"lxml\")\n",
    "\n",
    "        # find the unordered list (ul) elements which are the results, then\n",
    "        # find all href (a) tags that are from the base_url website.\n",
    "        index_links = bs_object.find(\"ul\", {\"data-testid\": \"results\"}).findAll(\n",
    "            \"a\", href=re.compile(f\"{BASE_URL}/*\")  # the `*` denotes wildcard any\n",
    "        )\n",
    "\n",
    "        for link in index_links:\n",
    "            # if it's a property address, add it to the list\n",
    "            if 'address' in link.get('class', []):\n",
    "                url_links.append(link['href'])\n",
    "\n",
    "    # for each url, scrape some basic metadata\n",
    "    pbar = tqdm(url_links)\n",
    "    success_count, total_count = 0, 0\n",
    "\n",
    "    for property_url in pbar:\n",
    "        try:\n",
    "            bs_object = BeautifulSoup(urlopen(Request(property_url, headers={'User-Agent': \"PostmanRuntime/7.6.0\"})), \"lxml\")\n",
    "            total_count += 1\n",
    "\n",
    "            property_page = urlopen(Request(property_url, headers={'User-Agent': \"PostmanRuntime/7.6.0\"}))\n",
    "            property_soup = BeautifulSoup(property_page, \"lxml\")\n",
    "\n",
    "            # looks for the header class to get property name\n",
    "            property_metadata[property_url]['name'] = bs_object.find(\"h1\", {\"class\": \"css-164r41r\"}).text.strip()\n",
    "\n",
    "            # looks for the div containing a summary title for cost\n",
    "            property_metadata[property_url]['cost_text'] = bs_object.find(\n",
    "                \"div\", {\"data-testid\": \"listing-details__summary-title\"}\n",
    "            ).text.strip()\n",
    "\n",
    "\n",
    "            # get rooms and parking\n",
    "            rooms = bs_object.find(\"div\", {\"data-testid\": \"property-features\"}).findAll(\n",
    "                \"span\", {\"data-testid\": \"property-features-text-container\"}\n",
    "            )\n",
    "\n",
    "            # rooms\n",
    "            property_metadata[property_url]['rooms'] = \", \".join(\n",
    "                [re.findall(r'\\d+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'Bed' in feature.text or 'Bath' in feature.text]\n",
    "            )\n",
    "\n",
    "            # parking\n",
    "            property_metadata[property_url]['parking'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'Parking' in feature.text]\n",
    "            )\n",
    "\n",
    "            # desc\n",
    "            property_metadata[property_url]['desc'] = \", \".join(\n",
    "                [re.findall(r'\\d+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'desc' in feature.text]\n",
    "            )\n",
    "            \n",
    "            # listingID:\n",
    "            property_metadata[property_url]['listingid'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'listingId' in feature.text]\n",
    "            )\n",
    "\n",
    "            # street:\n",
    "            property_metadata[property_url]['street'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'street' in feature.text]\n",
    "            )\n",
    "\n",
    "            # suburb:\n",
    "            property_metadata[property_url]['suburb'] = extract_suburb(property_metadata[property_url]['name'])\n",
    "\n",
    "            \n",
    "            # postcode:\n",
    "            property_metadata[property_url]['postcode'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'postcode' in feature.text]\n",
    "            )\n",
    "\n",
    "            # property type:\n",
    "            property_metadata[property_url]['propertyType'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'apartment' in feature.text \n",
    "                 or 'unit' in feature.text or 'house' in feature.text or 'flat' in feature.text]\n",
    "            )\n",
    "\n",
    "            # schools:\n",
    "            property_metadata[property_url]['school'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'school' in feature.text]\n",
    "            )\n",
    "\n",
    "            # features:\n",
    "            property_metadata[property_url]['features'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'feature' in feature.text]\n",
    "            )\n",
    "\n",
    "            # loanfinder:\n",
    "            property_metadata[property_url]['loan'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'loan' in feature.text]\n",
    "            )\n",
    "\n",
    "            # listingSummary:\n",
    "            property_metadata[property_url]['listingsummary'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'summary' in feature.text]\n",
    "            )\n",
    "\n",
    "            # suburb insights:\n",
    "            property_metadata[property_url]['suburbInsights'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'suburbInsights' in feature.text]\n",
    "            )\n",
    "\n",
    "            # property description\n",
    "            property_metadata[property_url]['desc'] = bs_object.find(\"p\").text.strip() if bs_object.find(\"p\") else \"N/A\"\n",
    "\n",
    "\n",
    "            # Scrape property description\n",
    "            property_metadata[property_url]['desc'] = re.sub(r'<br\\/>', '\\n', str(property_soup.find(\"p\"))).strip('</p>')\n",
    "           \n",
    "            \"\"\"\n",
    "            # Write each row to the CSV\n",
    "            writer.writerow([\n",
    "                property_url,\n",
    "                property_metadata[property_url]['name'],\n",
    "                property_metadata[property_url]['cost_text'],\n",
    "                property_metadata[property_url]['rooms'],\n",
    "                property_metadata[property_url]['parking'],\n",
    "                property_metadata[property_url]['desc'],\n",
    "                property_metadata[property_url]['listingid'],\n",
    "                property_metadata[property_url]['street'],\n",
    "                property_metadata[property_url]['suburb'],\n",
    "                property_metadata[property_url]['postcode'],\n",
    "                property_metadata[property_url]['propertyType'],\n",
    "                property_metadata[property_url]['school'],\n",
    "                property_metadata[property_url]['features'],\n",
    "                property_metadata[property_url]['loan'],\n",
    "                property_metadata[property_url]['listingsummary'],\n",
    "                property_metadata[property_url]['suburbInsights']\n",
    "            ])\n",
    "            \"\"\"\n",
    "            success_count += 1\n",
    "            temp_sdf = spark.createDataFrame(property_metadata)\n",
    "            sdf.union(temp_sdf)\n",
    "\n",
    "        except AttributeError:\n",
    "            print(f\"Issue with {property_url}\")\n",
    "\n",
    "        pbar.set_description(f\"{(success_count / total_count * 100):.0f}% successful\")\n",
    "\n",
    "        # output to example json in data/raw/\n",
    "    with open('../data/raw/example.json', 'w') as f:\n",
    "        dump(property_metadata, f)\n",
    "\n",
    "def convert_to_parquet(filepath: str, output_path: str) -> None:\n",
    "    \"\"\" Function converts a json file into a parquet file\n",
    "\n",
    "    Parameters:\n",
    "    filepath (str): the filepath that locates our json data\n",
    "\n",
    "    output_path (str): the filepath that we will place our new parquet file into\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    with open(filepath) as f:\n",
    "        data = load(f)\n",
    "\n",
    "    new_data = change_json_format(data)\n",
    "\n",
    "    # conversion from json -> dataframe -> parquet\n",
    "    df = pd.DataFrame(new_data)\n",
    "    df.to_parquet(output_path, engine='pyarrow')\n",
    "\n",
    "    delete_json_file(filepath)\n",
    "\n",
    "# function that changes the formatting of the json file\n",
    "def change_json_format(data: dict) -> dict:\n",
    "    \"\"\" Function grabs the renames the json keys to the words after the last backslash in the url and adds the url as an item\n",
    "\n",
    "    Parameters:\n",
    "    data (dict): json dictionary we are changing\n",
    "\n",
    "    Returns:\n",
    "    dict: our new json dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    new_data = {}\n",
    "    for i in data.keys():\n",
    "        new_name = i.rsplit('/', 1)[-1]\n",
    "        new_data[new_name] = data[i]\n",
    "        new_data[new_name][\"href\"] = i\n",
    "    return new_data\n",
    "\n",
    "def delete_json_file(filepath: str) -> None:\n",
    "    \"\"\" Function deletes the json file we are converting from\n",
    "\n",
    "    Parameters:\n",
    "    filepath (string): filepath to the json file we are deleting\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.remove(filepath)\n",
    "        print(f\"File '{filepath}' deleted successfully\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{filepath}' not found\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: '{filepath}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/09/12 10:55:12 WARN Utils: Your hostname, DESKTOP-RBVA59Q resolves to a loopback address: 127.0.1.1; using 172.26.88.196 instead (on interface eth0)\n",
      "24/09/12 10:55:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/12 10:55:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "from pyarrow import json\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_parquet(filepath: str, output_path: str) -> None:\n",
    "    \"\"\" Function converts a JSON file into a parquet file \"\"\"\n",
    "    with open(filepath) as f:\n",
    "        data = load(f)\n",
    "\n",
    "    new_data = change_json_format(data)\n",
    "\n",
    "    # Conversion from JSON -> DataFrame -> Parquet\n",
    "    df = pd.DataFrame(new_data)\n",
    "    df.to_parquet(output_path, engine='pyarrow')\n",
    "\n",
    "    delete_json_file(filepath)\n",
    "\n",
    "def change_json_format(data: dict) -> dict:\n",
    "    \"\"\" Function renames JSON keys and adds the URL as an item \"\"\"\n",
    "    new_data = {}\n",
    "    for i in data.keys():\n",
    "        new_name = i.rsplit('/', 1)[-1]\n",
    "        new_data[new_name] = data[i]\n",
    "        new_data[new_name][\"href\"] = i\n",
    "    return new_data\n",
    "\n",
    "def delete_json_file(filepath: str) -> None:\n",
    "    \"\"\" Function deletes the JSON file \"\"\"\n",
    "    try:\n",
    "        os.remove(filepath)\n",
    "        print(f\"File '{filepath}' deleted successfully\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{filepath}' not found\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: '{filepath}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "def get_chunks(suburbs_df) -> dict:\n",
    "    \"\"\"function that splits up postcodes into chunks of 50 so that if we are kicked halfway during scraping we don't lose too much progress\n",
    "    \"\"\"\n",
    "    i = 3050\n",
    "    j = 3000\n",
    "    chunk_dict = {}\n",
    "    while i < 4000:\n",
    "        temp = suburbs_df[suburbs_df['postcode'] >= j]\n",
    "        chunk_dict['chunk_{}'.format(i)] = temp[temp['postcode'] < i]\n",
    "        i += 50\n",
    "        j += 50\n",
    "    return chunk_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run cell above\n",
    "2. Run cell below \n",
    "3. Run cell below the cell below\n",
    "4. Run property_metadata.write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/09/13 10:56:06 WARN Utils: Your hostname, DESKTOP-RBVA59Q resolves to a loopback address: 127.0.1.1; using 172.26.88.196 instead (on interface eth0)\n",
      "24/09/13 10:56:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/13 10:56:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Working METHOD\n",
    "import re\n",
    "from json import dump\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.master('local[*]') \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .appName(\"PropertyScraper\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://www.domain.com.au\"\n",
    "N_PAGES = range(1, 50)  # Max number of pages you want to scrape\n",
    "\n",
    "# Load suburbs CSV\n",
    "suburbs_df = pd.read_csv('postcodes.csv')  # Ensure this CSV contains 'suburb' and 'postcode' columns\n",
    "chunk_dict = get_chunks(suburbs_df)\n",
    "\n",
    "def start_scrape(chunk, file_suffix):\n",
    "    \"\"\"Function that scrapes https://www.domain.com.au and outputs the data into a JSON file\n",
    "    \n",
    "    parameters:\n",
    "    chunk: chunk of 50 postcodes we will scrape\n",
    "    file_suffix: what we want to title the end of our files when we write to json\n",
    "    \"\"\"\n",
    "\n",
    "    # Define schema for the Spark DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"url\", StringType(), True),\n",
    "        StructField(\"postcode\", StringType(), True),\n",
    "        StructField(\"suburb\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"cost_text\", StringType(), True),\n",
    "        StructField(\"beds\", StringType(), True),  # Separate field for beds\n",
    "        StructField(\"baths\", StringType(), True),  # Separate field for baths\n",
    "        StructField(\"parking\", StringType(), True),  # Parking field\n",
    "        StructField(\"property_type\", StringType(), True),  # Property type field\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Initialize an empty DataFrame with the schema\n",
    "    property_metadata = spark.createDataFrame([], schema)\n",
    "\n",
    "    # Loop through each suburb and its postcode\n",
    "    for index, row in chunk.iterrows():\n",
    "        suburb = row['locality'].lower().replace(' ', '-')  # Convert to lowercase and hyphenate\n",
    "        postcode = row['postcode']\n",
    "\n",
    "        print(f\"Scraping data for {suburb} ({postcode})\")\n",
    "\n",
    "        url_links = []\n",
    "        page_found = False  # This flag will help us track whether any results are found\n",
    "\n",
    "        # Generate list of URLs to visit\n",
    "        for page in N_PAGES:\n",
    "            url = BASE_URL + f\"/rent/{suburb}-vic-{postcode}/?ssubs=0&sort=suburb-asc&page={page}\"\n",
    "            try:\n",
    "                bs_object = BeautifulSoup(urlopen(Request(url, headers={'User-Agent': \"PostmanRuntime/7.6.0\"})), \"lxml\")\n",
    "\n",
    "                # Check if the page has results or shows \"No results found\"\n",
    "                no_results = bs_object.find(text=re.compile(\"No results found\", re.I))\n",
    "                if no_results:\n",
    "                    print(f\"No results found for {suburb} on page {page}. Stopping further scraping for this suburb.\")\n",
    "                    break  # Exit the pagination loop if no results are found\n",
    "\n",
    "                # Find property links\n",
    "                index_links = bs_object.find(\"ul\", {\"data-testid\": \"results\"})\n",
    "                if not index_links:\n",
    "                    print(f\"No more results for {suburb} on page {page}.\")\n",
    "                    break  # Exit pagination if no results list is found (end of pages)\n",
    "\n",
    "                index_links = index_links.findAll(\"a\", href=re.compile(f\"{BASE_URL}/*\"))\n",
    "                page_found = True  # At least one result was found on this page\n",
    "\n",
    "                for link in index_links:\n",
    "                    # If it's a property address, add it to the list\n",
    "                    if 'address' in link.get('class', []):\n",
    "                        url_links.append(link['href'])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {url}: {e}\")\n",
    "                break  # Stop if there's an issue with fetching the page\n",
    "\n",
    "        if not page_found:\n",
    "            print(f\"No results for {suburb}. Moving to the next suburb.\")\n",
    "            continue  # Skip to the next suburb if no pages were found for this one\n",
    "\n",
    "        # For each URL, scrape some basic metadata\n",
    "        pbar = tqdm(url_links)\n",
    "        success_count, total_count = 0, 0\n",
    "\n",
    "        for property_url in pbar:\n",
    "            try:\n",
    "                bs_object = BeautifulSoup(urlopen(Request(property_url, headers={'User-Agent': \"PostmanRuntime/7.6.0\"})), \"lxml\")\n",
    "                total_count += 1\n",
    "\n",
    "                # Get property name\n",
    "                name = bs_object.find(\"h1\", {\"class\": \"css-164r41r\"}).text.strip()\n",
    "\n",
    "                # Get cost text\n",
    "                cost_text = bs_object.find(\"div\", {\"data-testid\": \"listing-details__summary-title\"}).text.strip()\n",
    "\n",
    "                # Get rooms (beds and baths)\n",
    "                rooms = bs_object.find(\"div\", {\"data-testid\": \"property-features\"}).findAll(\n",
    "                    \"span\", {\"data-testid\": \"property-features-text-container\"}\n",
    "                )\n",
    "\n",
    "                # Initialize variables\n",
    "                beds, baths, parking = None, None, '0'  # Default value for parking is '0 Car'\n",
    "\n",
    "                for feature in rooms:\n",
    "                    text = feature.text\n",
    "                    if 'Bed' in text:\n",
    "                        beds_match = re.findall(r'\\d+', text)\n",
    "                        if beds_match:\n",
    "                            beds = beds_match[0]  # Extract the number of beds\n",
    "                    elif 'Bath' in text:\n",
    "                        baths_match = re.findall(r'\\d+', text)\n",
    "                        if baths_match:\n",
    "                            baths = baths_match[0]  # Extract the number of baths\n",
    "                    elif 'Car' in text or 'Parking' in text:\n",
    "                        parking_match = re.findall(r'\\d+', text)\n",
    "                        if parking_match:\n",
    "                            parking = parking_match[0]  # Extract the number of parking spaces\n",
    "\n",
    "                property_type_container = bs_object.find(\"div\", {\"data-testid\": \"listing-summary-property-type\"})\n",
    "                property_type = property_type_container.get_text(strip=True)\n",
    "\n",
    "                # Create a row and append it to the DataFrame\n",
    "                row = [(property_url, postcode, suburb, name, cost_text, beds, baths, parking, property_type)]\n",
    "                row_df = spark.createDataFrame(row, schema)\n",
    "                property_metadata = property_metadata.union(row_df)\n",
    "                success_count += 1\n",
    "\n",
    "            except AttributeError:\n",
    "                print(f\"Error scraping {property_url}: missing data\")\n",
    "\n",
    "            pbar.set_description(f\"{(success_count / total_count * 100):.0f}% successful\")\n",
    "\n",
    "        # Show the DataFrame to ensure data is being appended\n",
    "        #property_metadata.show()\n",
    "\n",
    "    # Output to parquet file\n",
    "    #try:\n",
    "     #   property_metadata.write.mode(\"overwrite\").json('../data/raw/work_{}.json'.format(file_suffix))\n",
    "      #  print(f\"Data successfully written\")\n",
    "    #except Exception as e:\n",
    "     #   print(f\"An error occured: {e}\")\n",
    "\n",
    "    #added this print statement so that the cell output can be scrollable - it's getting annoying to click the scroll bar >:(\n",
    "    print(\"chunk finished\")\n",
    "    return property_metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for melbourne (3000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_501771/1003962613.py:69: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  no_results = bs_object.find(text=re.compile(\"No results found\", re.I))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more results for melbourne on page 45.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 874/874 [12:43<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for melbourne (3001)\n",
      "Error fetching https://www.domain.com.au/rent/melbourne-vic-3001/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for melbourne. Moving to the next suburb.\n",
      "Scraping data for east-melbourne (3002)\n",
      "No more results for east-melbourne on page 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 37/37 [00:31<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for west-melbourne (3003)\n",
      "No more results for west-melbourne on page 6.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 100/100 [01:26<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for melbourne (3004)\n",
      "No more results for melbourne on page 6.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 94/94 [01:25<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for st-kilda-road-central (3004)\n",
      "Error fetching https://www.domain.com.au/rent/st-kilda-road-central-vic-3004/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for st-kilda-road-central. Moving to the next suburb.\n",
      "Scraping data for st-kilda-road-melbourne (3004)\n",
      "Error fetching https://www.domain.com.au/rent/st-kilda-road-melbourne-vic-3004/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for st-kilda-road-melbourne. Moving to the next suburb.\n",
      "Scraping data for world-trade-centre (3005)\n",
      "Error fetching https://www.domain.com.au/rent/world-trade-centre-vic-3005/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for world-trade-centre. Moving to the next suburb.\n",
      "Scraping data for south-wharf (3006)\n",
      "No more results for south-wharf on page 1.\n",
      "No results for south-wharf. Moving to the next suburb.\n",
      "Scraping data for southbank (3006)\n",
      "No more results for southbank on page 21.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 398/398 [05:32<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for docklands (3008)\n",
      "No more results for docklands on page 11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 199/199 [02:41<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for university-of-melbourne (3010)\n",
      "Error fetching https://www.domain.com.au/rent/university-of-melbourne-vic-3010/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for university-of-melbourne. Moving to the next suburb.\n",
      "Scraping data for footscray (3011)\n",
      "No more results for footscray on page 9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 143/143 [01:59<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for seddon (3011)\n",
      "No more results for seddon on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 15/15 [00:16<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for seddon-west (3011)\n",
      "Error fetching https://www.domain.com.au/rent/seddon-west-vic-3011/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for seddon-west. Moving to the next suburb.\n",
      "Scraping data for brooklyn (3012)\n",
      "No more results for brooklyn on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for kingsville (3012)\n",
      "No more results for kingsville on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 7/7 [00:04<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for kingsville-west (3012)\n",
      "Error fetching https://www.domain.com.au/rent/kingsville-west-vic-3012/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for kingsville-west. Moving to the next suburb.\n",
      "Scraping data for maidstone (3012)\n",
      "No more results for maidstone on page 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 34/34 [00:26<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for tottenham (3012)\n",
      "No more results for tottenham on page 1.\n",
      "No results for tottenham. Moving to the next suburb.\n",
      "Scraping data for west-footscray (3012)\n",
      "No more results for west-footscray on page 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 34/34 [00:34<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for yarraville (3013)\n",
      "No more results for yarraville on page 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 56/56 [00:49<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for yarraville-west (3013)\n",
      "Error fetching https://www.domain.com.au/rent/yarraville-west-vic-3013/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for yarraville-west. Moving to the next suburb.\n",
      "Scraping data for newport (3015)\n",
      "No more results for newport on page 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 26/26 [00:20<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for south-kingsville (3015)\n",
      "No more results for south-kingsville on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 11/11 [00:10<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for spotswood (3015)\n",
      "No more results for spotswood on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 12/12 [00:10<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for williamstown (3016)\n",
      "No more results for williamstown on page 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 43/43 [00:36<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for williamstown-north (3016)\n",
      "No more results for williamstown-north on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 4/4 [00:06<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for altona (3018)\n",
      "No more results for altona on page 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 26/26 [00:21<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for seaholme (3018)\n",
      "No more results for seaholme on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for braybrook (3019)\n",
      "No more results for braybrook on page 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 22/22 [00:17<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for braybrook-north (3019)\n",
      "Error fetching https://www.domain.com.au/rent/braybrook-north-vic-3019/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for braybrook-north. Moving to the next suburb.\n",
      "Scraping data for robinson (3019)\n",
      "Error fetching https://www.domain.com.au/rent/robinson-vic-3019/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for robinson. Moving to the next suburb.\n",
      "Scraping data for albion (3020)\n",
      "No more results for albion on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 20/20 [00:14<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for glengala (3020)\n",
      "Error fetching https://www.domain.com.au/rent/glengala-vic-3020/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for glengala. Moving to the next suburb.\n",
      "Scraping data for sunshine (3020)\n",
      "No more results for sunshine on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 19/19 [00:16<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for sunshine-north (3020)\n",
      "No more results for sunshine-north on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 13/13 [00:09<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for sunshine-west (3020)\n",
      "No more results for sunshine-west on page 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 26/26 [00:19<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for albanvale (3021)\n",
      "No more results for albanvale on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 3/3 [00:02<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for kealba (3021)\n",
      "No more results for kealba on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 2/2 [00:01<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for kings-park (3021)\n",
      "No more results for kings-park on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 6/6 [00:04<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for st-albans (3021)\n",
      "No more results for st-albans on page 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 41/41 [00:31<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for ardeer (3022)\n",
      "No more results for ardeer on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 5/5 [00:04<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for deer-park-east (3022)\n",
      "Error fetching https://www.domain.com.au/rent/deer-park-east-vic-3022/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for deer-park-east. Moving to the next suburb.\n",
      "Scraping data for burnside (3023)\n",
      "No more results for burnside on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 3/3 [00:02<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for burnside-heights (3023)\n",
      "No more results for burnside-heights on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 4/4 [00:06<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for cairnlea (3023)\n",
      "No more results for cairnlea on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 5/5 [00:03<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for caroline-springs (3023)\n",
      "No more results for caroline-springs on page 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 24/24 [00:19<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for deer-park (3023)\n",
      "No more results for deer-park on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 10/10 [00:07<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for deer-park-north (3023)\n",
      "Error fetching https://www.domain.com.au/rent/deer-park-north-vic-3023/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for deer-park-north. Moving to the next suburb.\n",
      "Scraping data for ravenhall (3023)\n",
      "No more results for ravenhall on page 1.\n",
      "No results for ravenhall. Moving to the next suburb.\n",
      "Scraping data for fieldstone (3024)\n",
      "No more results for fieldstone on page 1.\n",
      "No results for fieldstone. Moving to the next suburb.\n",
      "Scraping data for mambourin (3024)\n",
      "No more results for mambourin on page 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 28/28 [00:23<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for manor-lakes (3024)\n",
      "No more results for manor-lakes on page 5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 67/67 [00:55<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for mount-cottrell (3024)\n",
      "No more results for mount-cottrell on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 1/1 [00:02<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for wyndham-vale (3024)\n",
      "No more results for wyndham-vale on page 7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 101/101 [01:20<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for altona-east (3025)\n",
      "No more results for altona-east on page 1.\n",
      "No results for altona-east. Moving to the next suburb.\n",
      "Scraping data for altona-gate (3025)\n",
      "Error fetching https://www.domain.com.au/rent/altona-gate-vic-3025/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for altona-gate. Moving to the next suburb.\n",
      "Scraping data for altona-north (3025)\n",
      "No more results for altona-north on page 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 35/35 [00:29<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for derrimut (3026)\n",
      "No more results for derrimut on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 8/8 [00:07<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for laverton-north (3026)\n",
      "No more results for laverton-north on page 1.\n",
      "No results for laverton-north. Moving to the next suburb.\n",
      "Scraping data for laverton-raaf (3027)\n",
      "Error fetching https://www.domain.com.au/rent/laverton-raaf-vic-3027/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for laverton-raaf. Moving to the next suburb.\n",
      "Scraping data for williams-landing (3027)\n",
      "No more results for williams-landing on page 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 27/27 [00:26<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for williams-raaf (3027)\n",
      "Error fetching https://www.domain.com.au/rent/williams-raaf-vic-3027/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for williams-raaf. Moving to the next suburb.\n",
      "Scraping data for altona-meadows (3028)\n",
      "No more results for altona-meadows on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 17/17 [00:24<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for laverton (3028)\n",
      "No more results for laverton on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 20/20 [00:13<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for seabrook (3028)\n",
      "No more results for seabrook on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 6/6 [00:03<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for hoppers-crossing (3029)\n",
      "No more results for hoppers-crossing on page 5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 64/64 [00:53<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for tarneit (3029)\n",
      "No more results for tarneit on page 11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 188/188 [02:37<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for truganina (3029)\n",
      "No more results for truganina on page 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 165/165 [02:29<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for chartwell (3030)\n",
      "Error fetching https://www.domain.com.au/rent/chartwell-vic-3030/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for chartwell. Moving to the next suburb.\n",
      "Scraping data for cocoroc (3030)\n",
      "No more results for cocoroc on page 1.\n",
      "No results for cocoroc. Moving to the next suburb.\n",
      "Scraping data for point-cook (3030)\n",
      "No more results for point-cook on page 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 139/139 [01:51<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for quandong (3030)\n",
      "No more results for quandong on page 1.\n",
      "No results for quandong. Moving to the next suburb.\n",
      "Scraping data for werribee (3030)\n",
      "No more results for werribee on page 7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 112/112 [01:37<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for werribee-south (3030)\n",
      "No more results for werribee-south on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 12/12 [00:09<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for flemington (3031)\n",
      "No more results for flemington on page 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 28/28 [00:27<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for kensington (3031)\n",
      "No more results for kensington on page 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 43/43 [00:37<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for ascot-vale (3032)\n",
      "No more results for ascot-vale on page 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 37/37 [00:34<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for highpoint-city (3032)\n",
      "Error fetching https://www.domain.com.au/rent/highpoint-city-vic-3032/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for highpoint-city. Moving to the next suburb.\n",
      "Scraping data for maribyrnong (3032)\n",
      "No more results for maribyrnong on page 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 49/49 [00:50<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for travancore (3032)\n",
      "No more results for travancore on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 14/14 [00:12<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for keilor-east (3033)\n",
      "No more results for keilor-east on page 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 24/24 [00:19<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for avondale-heights (3034)\n",
      "No more results for avondale-heights on page 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 29/29 [00:27<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for keilor (3036)\n",
      "No more results for keilor on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 3/3 [00:02<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for keilor-north (3036)\n",
      "No more results for keilor-north on page 1.\n",
      "No results for keilor-north. Moving to the next suburb.\n",
      "Scraping data for calder-park (3037)\n",
      "No more results for calder-park on page 1.\n",
      "No results for calder-park. Moving to the next suburb.\n",
      "Scraping data for delahey (3037)\n",
      "No more results for delahey on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 5/5 [00:05<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for hillside (3037)\n",
      "No more results for hillside on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 9/9 [00:08<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for plumpton (3037)\n",
      "Error fetching https://www.domain.com.au/rent/plumpton-vic-3037/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for plumpton. Moving to the next suburb.\n",
      "Scraping data for sydenham (3037)\n",
      "No more results for sydenham on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 11/11 [00:09<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for taylors-hill (3037)\n",
      "No more results for taylors-hill on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 4/4 [00:04<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for keilor-downs (3038)\n",
      "No more results for keilor-downs on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 8/8 [00:06<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for keilor-lodge (3038)\n",
      "No more results for keilor-lodge on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for taylors-lakes (3038)\n",
      "No more results for taylors-lakes on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 4/4 [00:04<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for watergardens (3038)\n",
      "Error fetching https://www.domain.com.au/rent/watergardens-vic-3038/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for watergardens. Moving to the next suburb.\n",
      "Scraping data for moonee-ponds (3039)\n",
      "No more results for moonee-ponds on page 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 40/40 [00:43<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for aberfeldie (3040)\n",
      "No more results for aberfeldie on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 9/9 [00:09<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for essendon (3040)\n",
      "No more results for essendon on page 5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 70/70 [01:05<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for essendon-west (3040)\n",
      "No more results for essendon-west on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 6/6 [00:04<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for cross-keys (3041)\n",
      "Error fetching https://www.domain.com.au/rent/cross-keys-vic-3041/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for cross-keys. Moving to the next suburb.\n",
      "Scraping data for essendon-fields (3041)\n",
      "No more results for essendon-fields on page 1.\n",
      "No results for essendon-fields. Moving to the next suburb.\n",
      "Scraping data for essendon-north (3041)\n",
      "No more results for essendon-north on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 16/16 [00:17<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for strathmore (3041)\n",
      "No more results for strathmore on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 14/14 [00:16<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for strathmore-heights (3041)\n",
      "No more results for strathmore-heights on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for airport-west (3042)\n",
      "No more results for airport-west on page 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 22/22 [00:27<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for keilor-park (3042)\n",
      "No more results for keilor-park on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for niddrie (3042)\n",
      "No more results for niddrie on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 11/11 [00:12<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for niddrie-north (3042)\n",
      "Error fetching https://www.domain.com.au/rent/niddrie-north-vic-3042/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for niddrie-north. Moving to the next suburb.\n",
      "Scraping data for gladstone-park (3043)\n",
      "No more results for gladstone-park on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 7/7 [00:09<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for gowanbrae (3043)\n",
      "No more results for gowanbrae on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 2/2 [00:01<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for tullamarine (3043)\n",
      "No more results for tullamarine on page 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 23/23 [00:22<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for pascoe-vale (3044)\n",
      "No more results for pascoe-vale on page 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 40/40 [00:40<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for pascoe-vale-south (3044)\n",
      "No more results for pascoe-vale-south on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 12/12 [00:12<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for melbourne-airport (3045)\n",
      "No more results for melbourne-airport on page 1.\n",
      "No results for melbourne-airport. Moving to the next suburb.\n",
      "Scraping data for glenroy (3046)\n",
      "No more results for glenroy on page 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 60/60 [01:03<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for hadfield (3046)\n",
      "No more results for hadfield on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 7/7 [00:06<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for oak-park (3046)\n",
      "No more results for oak-park on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 12/12 [00:10<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for broadmeadows (3047)\n",
      "No more results for broadmeadows on page 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 23/23 [00:25<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for dallas (3047)\n",
      "No more results for dallas on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 4/4 [00:04<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for jacana (3047)\n",
      "No more results for jacana on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for coolaroo (3048)\n",
      "No more results for coolaroo on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for meadow-heights (3048)\n",
      "No more results for meadow-heights on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 19/19 [00:22<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for attwood (3049)\n",
      "No more results for attwood on page 1.\n",
      "No results for attwood. Moving to the next suburb.\n",
      "Scraping data for westmeadows (3049)\n",
      "No more results for westmeadows on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 6/6 [00:07<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# start scraping by chunks of 50\n",
    "#for i in chunk_dict:\n",
    " #   start_scrape(chunk_dict[i], i.split(\"_\")[1])\n",
    "property_metadata = start_scrape(chunk_dict['chunk_3050'], '3050')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/13 10:45:58 WARN DAGScheduler: Broadcasting large task binary with size 28.3 MiB\n",
      "24/09/13 10:45:59 WARN DAGScheduler: Broadcasting large task binary with size 28.3 MiB\n",
      "24/09/13 10:46:00 WARN DAGScheduler: Broadcasting large task binary with size 28.3 MiB\n",
      "24/09/13 10:46:06 WARN DAGScheduler: Broadcasting large task binary with size 28.3 MiB\n",
      "[Stage 2:>                                                          (0 + 0) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+--------------------+-------------+----+-----+-------+--------------------+\n",
      "|                 url|postcode|   suburb|                name|    cost_text|beds|baths|parking|       property_type|\n",
      "+--------------------+--------+---------+--------------------+-------------+----+-----+-------+--------------------+\n",
      "|https://www.domai...|    3000|melbourne|1503/270 King Str...|$850 Per Week|   4|    1|      1|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|3408/138 Spencer ...|      $625 pw|   1|    1|      0|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|103/300 Swanston ...|      $620 pw|   1|    1|      1|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|201/23 Queens Roa...|         $600|   2|    1|      1|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|804/225 Elizabeth...|      $570.00|   2|    1|      0|Apartment / Unit ...|\n",
      "+--------------------+--------+---------+--------------------+-------------+----+-----+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 4) / 4]\r"
     ]
    }
   ],
   "source": [
    "property_metadata.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/13 12:02:38 WARN DAGScheduler: Broadcasting large task binary with size 24.8 MiB\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ulizuli/real_estate_data/real estate data/project-2-group-real-estate-industry-project-22/env/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ulizuli/real_estate_data/real estate data/project-2-group-real-estate-industry-project-22/env/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ulizuli/real_estate_data/real estate data/project-2-group-real-estate-industry-project-22/env/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ulizuli/real_estate_data/real estate data/project-2-group-real-estate-industry-project-22/env/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ulizuli/real_estate_data/real estate data/project-2-group-real-estate-industry-project-22/env/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ulizuli/real_estate_data/real estate data/project-2-group-real-estate-industry-project-22/env/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 55108)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/ulizuli/real_estate_data/real estate data/project-2-group-real-estate-industry-project-22/env/lib/python3.10/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/ulizuli/real_estate_data/real estate data/project-2-group-real-estate-industry-project-22/env/lib/python3.10/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/home/ulizuli/real_estate_data/real estate data/project-2-group-real-estate-industry-project-22/env/lib/python3.10/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/ulizuli/real_estate_data/real estate data/project-2-group-real-estate-industry-project-22/env/lib/python3.10/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o59850.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mproperty_metadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/raw/work_3050.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/real_estate_data/real estate data/project-2-group-real-estate-industry-project-22/env/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/real_estate_data/real estate data/project-2-group-real-estate-industry-project-22/env/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/real_estate_data/real estate data/project-2-group-real-estate-industry-project-22/env/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/real_estate_data/real estate data/project-2-group-real-estate-industry-project-22/env/lib/python3.10/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o59850.parquet"
     ]
    }
   ],
   "source": [
    "property_metadata.write.mode(\"overwrite\").parquet(\"../data/raw/work_3050.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'eventLog'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meventLog\u001b[49m\u001b[38;5;241m.\u001b[39mdir\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'eventLog'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = suburbs_df[suburbs_df['postcode'] >= 3950]\n",
    "chunk_dict['chunk_3997'] = temp[temp['postcode'] < 3997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mchunk_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchunk_3100\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for i in chunk_dict[1:]:\n",
    "    start_scrape(chunk_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schema = StructType([\n",
    "        StructField(\"url\", StringType(), True),\n",
    "        StructField(\"postcode\", StringType(), True),\n",
    "        StructField(\"suburb\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"cost_text\", StringType(), True),\n",
    "        StructField(\"beds\", StringType(), True),  # Separate field for beds\n",
    "        StructField(\"baths\", StringType(), True),  # Separate field for baths\n",
    "        StructField(\"parking\", StringType(), True),  # Parking field\n",
    "        StructField(\"property_type\", StringType(), True),  # Property type field\n",
    "    ])\n",
    "work = spark.read.schema(schema).json('../data/raw/work_3050.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+----+---------+----+-----+-------+-------------+\n",
      "|url|postcode|suburb|name|cost_text|beds|baths|parking|property_type|\n",
      "+---+--------+------+----+---------+----+-----+-------+-------------+\n",
      "+---+--------+------+----+---------+----+-----+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "work.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  url  postcode   suburb  \\\n",
      "0   https://www.domain.com.au/34-evadene-drive-tar...      3029  tarneit   \n",
      "1   https://www.domain.com.au/434-bethany-road-tar...      3029  tarneit   \n",
      "2   https://www.domain.com.au/58-antonio-road-tarn...      3029  tarneit   \n",
      "3   https://www.domain.com.au/12-lindeman-street-t...      3029  tarneit   \n",
      "4   https://www.domain.com.au/3-imatra-loop-tarnei...      3029  tarneit   \n",
      "5   https://www.domain.com.au/84-lucania-crescent-...      3029  tarneit   \n",
      "6   https://www.domain.com.au/8-keeping-terrace-ta...      3029  tarneit   \n",
      "7   https://www.domain.com.au/40-kamala-drive-tarn...      3029  tarneit   \n",
      "8   https://www.domain.com.au/48-riland-boulevard-...      3029  tarneit   \n",
      "9   https://www.domain.com.au/9-ceremony-drive-tar...      3029  tarneit   \n",
      "10  https://www.domain.com.au/11-ogawa-walk-tarnei...      3029  tarneit   \n",
      "11  https://www.domain.com.au/tarneit-vic-3029-151...      3029  tarneit   \n",
      "\n",
      "                                     name                 cost_text  beds  \\\n",
      "0      34 Evadene Drive, Tarneit VIC 3029             $620 per week     4   \n",
      "1      434 Bethany Road, Tarneit VIC 3029                      $520     3   \n",
      "2       58 Antonio Road, Tarneit VIC 3029             $580 Per Week     4   \n",
      "3    12 Lindeman Street, Tarneit VIC 3029                   $650 pw     4   \n",
      "4         3 Imatra Loop, Tarneit VIC 3029                      $570     4   \n",
      "5   84 Lucania Crescent, Tarneit VIC 3029             Contact Agent     4   \n",
      "6     8 Keeping Terrace, Tarneit VIC 3029            $ 500 PER WEEK     3   \n",
      "7       40 Kamala Drive, Tarneit VIC 3029                      $520     3   \n",
      "8   48 Riland Boulevard, Tarneit VIC 3029                      $520     3   \n",
      "9      9 Ceremony Drive, Tarneit VIC 3029                       610     4   \n",
      "10        11 Ogawa Walk, Tarneit VIC 3029                       530     3   \n",
      "11                       Tarneit VIC 3029  Rent2own with No Deposit     4   \n",
      "\n",
      "    baths  parking property_type  \n",
      "0       3        2         House  \n",
      "1       2        2         House  \n",
      "2       2        2         House  \n",
      "3       2        2         House  \n",
      "4       2        2         House  \n",
      "5       2        2         House  \n",
      "6       2        1         House  \n",
      "7       2        1         House  \n",
      "8       2        1         House  \n",
      "9       2        2         House  \n",
      "10      2        2         House  \n",
      "11      2        2         House  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the folder path containing JSON files\n",
    "folder_path = '../data/raw/work.json'\n",
    "\n",
    "# List all files in the directory\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Read each JSON file into a DataFrame\n",
    "for file in json_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    # Read JSON file\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "    # Append the DataFrame to the list\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(combined_df.head(12))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
