{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/14 16:42:27 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "## METHOD 1: convert dictionary to spark dataframe and append to initialized sdf\n",
    "# built-in imports\n",
    "import re\n",
    "from json import dump, load\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "import pandas as pd  \n",
    "import os\n",
    "# Import Spark modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Domain Scraper\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#### create a spark data frame\n",
    "\n",
    "# constants\n",
    "BASE_URL = \"https://www.domain.com.au\"\n",
    "N_PAGES = range(1, 2)  # Update this to your liking\n",
    "\n",
    "#Scrape suburb from the address\n",
    "def extract_suburb(address: str) -> str:\n",
    "    \"\"\"Extract the suburb name from the property address.\"\"\"\n",
    "    match = re.search(r'(?<=, )\\w+', address)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def start_scrape() -> None:\n",
    "    \"\"\" Function that scrapes https://www.domain.com.au and outputs the data into a json file\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    schema = StructType([\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"cost_text\", StringType(), True),\n",
    "    StructField(\"rooms\", StringType(), True),\n",
    "    StructField(\"desc\", StringType(), True),\n",
    "    StructField(\"parking\", StringType(), True),\n",
    "    StructField(\"street\", StringType(), True),\n",
    "    StructField(\"suburb\", StringType(), True),\n",
    "    StructField(\"postcode\", StringType(), True),\n",
    "    StructField(\"propertyType\", StringType(), True),\n",
    "    StructField(\"school\", StringType(), True),\n",
    "    StructField(\"features\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "    # begin code\n",
    "    url_links = []\n",
    "    property_metadata = defaultdict(dict)\n",
    "    sdf = spark.createDataFrame([],schema)\n",
    "    \n",
    "\n",
    "    # generate list of urls to visit\n",
    "    for page in N_PAGES:\n",
    "        url = BASE_URL + f\"/rent/?excludedeposittaken=1&state=vic&page={page}\"\n",
    "        print(f\"Visiting {url}\")\n",
    "        bs_object = BeautifulSoup(urlopen(Request(url, headers={'User-Agent': \"PostmanRuntime/7.6.0\"})), \"lxml\")\n",
    "\n",
    "        # find the unordered list (ul) elements which are the results, then\n",
    "        # find all href (a) tags that are from the base_url website.\n",
    "        index_links = bs_object.find(\"ul\", {\"data-testid\": \"results\"}).findAll(\n",
    "            \"a\", href=re.compile(f\"{BASE_URL}/*\")  # the `*` denotes wildcard any\n",
    "        )\n",
    "\n",
    "        for link in index_links:\n",
    "            # if it's a property address, add it to the list\n",
    "            if 'address' in link.get('class', []):\n",
    "                url_links.append(link['href'])\n",
    "\n",
    "    # for each url, scrape some basic metadata\n",
    "    pbar = tqdm(url_links)\n",
    "    success_count, total_count = 0, 0\n",
    "\n",
    "    for property_url in pbar:\n",
    "        try:\n",
    "            bs_object = BeautifulSoup(urlopen(Request(property_url, headers={'User-Agent': \"PostmanRuntime/7.6.0\"})), \"lxml\")\n",
    "            total_count += 1\n",
    "\n",
    "            property_page = urlopen(Request(property_url, headers={'User-Agent': \"PostmanRuntime/7.6.0\"}))\n",
    "            property_soup = BeautifulSoup(property_page, \"lxml\")\n",
    "\n",
    "            # looks for the header class to get property name\n",
    "            property_metadata[property_url]['name'] = bs_object.find(\"h1\", {\"class\": \"css-164r41r\"}).text.strip()\n",
    "\n",
    "            # looks for the div containing a summary title for cost\n",
    "            property_metadata[property_url]['cost_text'] = bs_object.find(\n",
    "                \"div\", {\"data-testid\": \"listing-details__summary-title\"}\n",
    "            ).text.strip()\n",
    "\n",
    "\n",
    "            # get rooms and parking\n",
    "            rooms = bs_object.find(\"div\", {\"data-testid\": \"property-features\"}).findAll(\n",
    "                \"span\", {\"data-testid\": \"property-features-text-container\"}\n",
    "            )\n",
    "\n",
    "            # rooms\n",
    "            property_metadata[property_url]['rooms'] = \", \".join(\n",
    "                [re.findall(r'\\d+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'Bed' in feature.text or 'Bath' in feature.text]\n",
    "            )\n",
    "\n",
    "            # parking\n",
    "            property_metadata[property_url]['parking'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'Parking' in feature.text]\n",
    "            )\n",
    "\n",
    "            # desc\n",
    "            property_metadata[property_url]['desc'] = \", \".join(\n",
    "                [re.findall(r'\\d+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'desc' in feature.text]\n",
    "            )\n",
    "            \n",
    "            # listingID:\n",
    "            property_metadata[property_url]['listingid'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'listingId' in feature.text]\n",
    "            )\n",
    "\n",
    "            # street:\n",
    "            property_metadata[property_url]['street'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'street' in feature.text]\n",
    "            )\n",
    "\n",
    "            # suburb:\n",
    "            property_metadata[property_url]['suburb'] = extract_suburb(property_metadata[property_url]['name'])\n",
    "\n",
    "            \n",
    "            # postcode:\n",
    "            property_metadata[property_url]['postcode'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'postcode' in feature.text]\n",
    "            )\n",
    "\n",
    "            # property type:\n",
    "            property_metadata[property_url]['propertyType'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'apartment' in feature.text \n",
    "                 or 'unit' in feature.text or 'house' in feature.text or 'flat' in feature.text]\n",
    "            )\n",
    "\n",
    "            # schools:\n",
    "            property_metadata[property_url]['school'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'school' in feature.text]\n",
    "            )\n",
    "\n",
    "            # features:\n",
    "            property_metadata[property_url]['features'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'feature' in feature.text]\n",
    "            )\n",
    "\n",
    "            # loanfinder:\n",
    "            property_metadata[property_url]['loan'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'loan' in feature.text]\n",
    "            )\n",
    "\n",
    "            # listingSummary:\n",
    "            property_metadata[property_url]['listingsummary'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'summary' in feature.text]\n",
    "            )\n",
    "\n",
    "            # suburb insights:\n",
    "            property_metadata[property_url]['suburbInsights'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'suburbInsights' in feature.text]\n",
    "            )\n",
    "\n",
    "            # property description\n",
    "            property_metadata[property_url]['desc'] = bs_object.find(\"p\").text.strip() if bs_object.find(\"p\") else \"N/A\"\n",
    "\n",
    "\n",
    "            # Scrape property description\n",
    "            property_metadata[property_url]['desc'] = re.sub(r'<br\\/>', '\\n', str(property_soup.find(\"p\"))).strip('</p>')\n",
    "           \n",
    "            \"\"\"\n",
    "            # Write each row to the CSV\n",
    "            writer.writerow([\n",
    "                property_url,\n",
    "                property_metadata[property_url]['name'],\n",
    "                property_metadata[property_url]['cost_text'],\n",
    "                property_metadata[property_url]['rooms'],\n",
    "                property_metadata[property_url]['parking'],\n",
    "                property_metadata[property_url]['desc'],\n",
    "                property_metadata[property_url]['listingid'],\n",
    "                property_metadata[property_url]['street'],\n",
    "                property_metadata[property_url]['suburb'],\n",
    "                property_metadata[property_url]['postcode'],\n",
    "                property_metadata[property_url]['propertyType'],\n",
    "                property_metadata[property_url]['school'],\n",
    "                property_metadata[property_url]['features'],\n",
    "                property_metadata[property_url]['loan'],\n",
    "                property_metadata[property_url]['listingsummary'],\n",
    "                property_metadata[property_url]['suburbInsights']\n",
    "            ])\n",
    "            \"\"\"\n",
    "            success_count += 1\n",
    "            temp_sdf = spark.createDataFrame(property_metadata)\n",
    "            sdf.union(temp_sdf)\n",
    "\n",
    "        except AttributeError:\n",
    "            print(f\"Issue with {property_url}\")\n",
    "\n",
    "        pbar.set_description(f\"{(success_count / total_count * 100):.0f}% successful\")\n",
    "\n",
    "        # output to example json in data/raw/\n",
    "    with open('../data/raw/example.json', 'w') as f:\n",
    "        dump(property_metadata, f)\n",
    "\n",
    "def convert_to_parquet(filepath: str, output_path: str) -> None:\n",
    "    \"\"\" Function converts a json file into a parquet file\n",
    "\n",
    "    Parameters:\n",
    "    filepath (str): the filepath that locates our json data\n",
    "\n",
    "    output_path (str): the filepath that we will place our new parquet file into\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    with open(filepath) as f:\n",
    "        data = load(f)\n",
    "\n",
    "    new_data = change_json_format(data)\n",
    "\n",
    "    # conversion from json -> dataframe -> parquet\n",
    "    df = pd.DataFrame(new_data)\n",
    "    df.to_parquet(output_path, engine='pyarrow')\n",
    "\n",
    "    delete_json_file(filepath)\n",
    "\n",
    "# function that changes the formatting of the json file\n",
    "def change_json_format(data: dict) -> dict:\n",
    "    \"\"\" Function grabs the renames the json keys to the words after the last backslash in the url and adds the url as an item\n",
    "\n",
    "    Parameters:\n",
    "    data (dict): json dictionary we are changing\n",
    "\n",
    "    Returns:\n",
    "    dict: our new json dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    new_data = {}\n",
    "    for i in data.keys():\n",
    "        new_name = i.rsplit('/', 1)[-1]\n",
    "        new_data[new_name] = data[i]\n",
    "        new_data[new_name][\"href\"] = i\n",
    "    return new_data\n",
    "\n",
    "def delete_json_file(filepath: str) -> None:\n",
    "    \"\"\" Function deletes the json file we are converting from\n",
    "\n",
    "    Parameters:\n",
    "    filepath (string): filepath to the json file we are deleting\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.remove(filepath)\n",
    "        print(f\"File '{filepath}' deleted successfully\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{filepath}' not found\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: '{filepath}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/14 16:42:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "from pyarrow import json\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_parquet(filepath: str, output_path: str) -> None:\n",
    "    \"\"\" Function converts a JSON file into a parquet file \"\"\"\n",
    "    with open(filepath) as f:\n",
    "        data = load(f)\n",
    "\n",
    "    new_data = change_json_format(data)\n",
    "\n",
    "    # Conversion from JSON -> DataFrame -> Parquet\n",
    "    df = pd.DataFrame(new_data)\n",
    "    df.to_parquet(output_path, engine='pyarrow')\n",
    "\n",
    "    delete_json_file(filepath)\n",
    "\n",
    "def change_json_format(data: dict) -> dict:\n",
    "    \"\"\" Function renames JSON keys and adds the URL as an item \"\"\"\n",
    "    new_data = {}\n",
    "    for i in data.keys():\n",
    "        new_name = i.rsplit('/', 1)[-1]\n",
    "        new_data[new_name] = data[i]\n",
    "        new_data[new_name][\"href\"] = i\n",
    "    return new_data\n",
    "\n",
    "def delete_json_file(filepath: str) -> None:\n",
    "    \"\"\" Function deletes the JSON file \"\"\"\n",
    "    try:\n",
    "        os.remove(filepath)\n",
    "        print(f\"File '{filepath}' deleted successfully\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{filepath}' not found\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: '{filepath}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "def get_chunks(suburbs_df) -> dict:\n",
    "    \"\"\"function that splits up postcodes into chunks of 50 so that if we are kicked halfway during scraping we don't lose too much progress\n",
    "    \"\"\"\n",
    "    chunk_dict = {}\n",
    "    \n",
    "    i = 3000\n",
    "    j = 3000  \n",
    "    while i < 4000:\n",
    "        temp = suburbs_df[suburbs_df['postcode'] >= j]\n",
    "        chunk_dict['chunk_{}'.format(i)] = temp[temp['postcode'] < i]\n",
    "        if j == 3000:\n",
    "            j += 1\n",
    "            i += 51\n",
    "        else : \n",
    "            j += 50 \n",
    "            i += 50\n",
    "\n",
    "    return chunk_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk_3000': Empty DataFrame\n",
      "Columns: [Unnamed: 0, postcode, locality]\n",
      "Index: [], 'chunk_3051':      Unnamed: 0  postcode                  locality\n",
      "1          6203      3001                 MELBOURNE\n",
      "2          6204      3002            EAST MELBOURNE\n",
      "3          6205      3003            WEST MELBOURNE\n",
      "4          6206      3004                 MELBOURNE\n",
      "5          6207      3004     ST KILDA ROAD CENTRAL\n",
      "..          ...       ...                       ...\n",
      "121        6323      3048                  COOLAROO\n",
      "122        6324      3048            MEADOW HEIGHTS\n",
      "123        6325      3049                   ATTWOOD\n",
      "124        6326      3049               WESTMEADOWS\n",
      "125        6327      3050  ROYAL MELBOURNE HOSPITAL\n",
      "\n",
      "[125 rows x 3 columns], 'chunk_3101':      Unnamed: 0  postcode              locality\n",
      "126        6328      3051           HOTHAM HILL\n",
      "127        6329      3051       NORTH MELBOURNE\n",
      "128        6330      3052  MELBOURNE UNIVERSITY\n",
      "129        6331      3052             PARKVILLE\n",
      "130        6332      3053               CARLTON\n",
      "..          ...       ...                   ...\n",
      "229        6431      3099         ARTHURS CREEK\n",
      "230        6432      3099        COTTLES BRIDGE\n",
      "231        6433      3099           HURSTBRIDGE\n",
      "232        6434      3099              NUTFIELD\n",
      "233        6435      3099            STRATHEWEN\n",
      "\n",
      "[108 rows x 3 columns], 'chunk_3151':      Unnamed: 0  postcode       locality\n",
      "234        6436      3101         COTHAM\n",
      "235        6437      3101            KEW\n",
      "236        6438      3102       KEW EAST\n",
      "237        6439      3103         BALWYN\n",
      "238        6440      3103    BALWYN EAST\n",
      "..          ...       ...            ...\n",
      "359        6561      3149       PINEWOOD\n",
      "360        6562      3149         SYNDAL\n",
      "361        6563      3150   BRANDON PARK\n",
      "362        6564      3150  GLEN WAVERLEY\n",
      "363        6565      3150  WHEELERS HILL\n",
      "\n",
      "[130 rows x 3 columns], 'chunk_3201':      Unnamed: 0  postcode          locality\n",
      "364        6566      3151      BURWOOD EAST\n",
      "365        6567      3151   BURWOOD HEIGHTS\n",
      "366        6568      3152  KNOX CITY CENTRE\n",
      "367        6569      3152         STUDFIELD\n",
      "368        6570      3152          WANTIRNA\n",
      "..          ...       ...               ...\n",
      "485        6687      3199   FRANKSTON SOUTH\n",
      "486        6688      3199          KARINGAL\n",
      "487        6689      3199   KARINGAL CENTRE\n",
      "488        6690      3200   FRANKSTON NORTH\n",
      "489        6691      3200      PINES FOREST\n",
      "\n",
      "[126 rows x 3 columns], 'chunk_3251':      Unnamed: 0  postcode      locality\n",
      "490        6692      3201  CARRUM DOWNS\n",
      "491        6693      3202    HEATHERTON\n",
      "492        6694      3204     BENTLEIGH\n",
      "493        6695      3204      MCKINNON\n",
      "494        6696      3204        ORMOND\n",
      "..          ...       ...           ...\n",
      "679        6881      3249       YEODENE\n",
      "680        6882      3250         COLAC\n",
      "681        6883      3250    COLAC EAST\n",
      "682        6884      3250    COLAC WEST\n",
      "683        6885      3250     ELLIMINYT\n",
      "\n",
      "[194 rows x 3 columns], 'chunk_3301':      Unnamed: 0  postcode         locality\n",
      "684        6886      3251            BEEAC\n",
      "685        6887      3251          CUNDARE\n",
      "686        6888      3251    CUNDARE NORTH\n",
      "687        6889      3251           EURACK\n",
      "688        6890      3251          WEERING\n",
      "..          ...       ...              ...\n",
      "833        7035      3294   VICTORIA POINT\n",
      "834        7036      3294  VICTORIA VALLEY\n",
      "835        7037      3294        WOODHOUSE\n",
      "836        7038      3300     BYADUK NORTH\n",
      "837        7039      3300         HAMILTON\n",
      "\n",
      "[154 rows x 3 columns], 'chunk_3351':       Unnamed: 0  postcode        locality\n",
      "838         7040      3301         BOCHARA\n",
      "839         7041      3301      BROADWATER\n",
      "840         7042      3301   BUCKLEY SWAMP\n",
      "841         7043      3301          BYADUK\n",
      "842         7044      3301    CROXTON EAST\n",
      "...          ...       ...             ...\n",
      "1049        7251      3350         NERRINA\n",
      "1050        7252      3350       NEWINGTON\n",
      "1051        7253      3350           REDAN\n",
      "1052        7254      3350   SOLDIERS HILL\n",
      "1053        7255      3350  SOVEREIGN HILL\n",
      "\n",
      "[216 rows x 3 columns], 'chunk_3401':       Unnamed: 0  postcode          locality\n",
      "1054        7256      3351          BERRINGA\n",
      "1055        7257      3351           BO PEEP\n",
      "1056        7258      3351        CAPE CLEAR\n",
      "1057        7259      3351          CARNGHAM\n",
      "1058        7260      3351         CHEPSTOWE\n",
      "...          ...       ...               ...\n",
      "1365        7567      3400      HORSHAM WEST\n",
      "1366        7568      3400  ST HELENS PLAINS\n",
      "1367        7569      3400           WARTOOK\n",
      "1368        7570      3400    WONWONDAH EAST\n",
      "1369        7571      3400   WONWONDAH SOUTH\n",
      "\n",
      "[316 rows x 3 columns], 'chunk_3451':       Unnamed: 0  postcode        locality\n",
      "1370        7572      3401      BLACKHEATH\n",
      "1371        7573      3401        BRIMPAEN\n",
      "1372        7574      3401      BUNGALALLY\n",
      "1373        7575      3401      CHERRYPOOL\n",
      "1374        7576      3401      CLEAR LAKE\n",
      "...          ...       ...             ...\n",
      "1523        7725      3448     ELPHINSTONE\n",
      "1524        7726      3448        METCALFE\n",
      "1525        7727      3448   SUTTON GRANGE\n",
      "1526        7728      3450     CASTLEMAINE\n",
      "1527        7729      3450  MOONLIGHT FLAT\n",
      "\n",
      "[158 rows x 3 columns], 'chunk_3501':       Unnamed: 0  postcode           locality\n",
      "1528        7730      3451      BARKERS CREEK\n",
      "1529        7731      3451    CAMPBELLS CREEK\n",
      "1530        7732      3451            CHEWTON\n",
      "1531        7733      3451  CHEWTON BUSHLANDS\n",
      "1532        7734      3451            FARADAY\n",
      "...          ...       ...                ...\n",
      "1792        7994      3496          WERRIMULL\n",
      "1793        7995      3498            IRYMPLE\n",
      "1794        7996      3500            MILDURA\n",
      "1795        7997      3500       MILDURA EAST\n",
      "1796        7998      3500       MILDURA WEST\n",
      "\n",
      "[269 rows x 3 columns], 'chunk_3551':       Unnamed: 0  postcode              locality\n",
      "1797        8000      3501                HATTAH\n",
      "1798        8001      3501              KOORLONG\n",
      "1799        8002      3501  MILDURA CENTRE PLAZA\n",
      "1800        8003      3501         MILDURA SOUTH\n",
      "1801        8004      3501         NICHOLS POINT\n",
      "...          ...       ...                   ...\n",
      "1977        8180      3550          SPRING GULLY\n",
      "1978        8181      3550            STRATHDALE\n",
      "1979        8182      3550           TYSONS REEF\n",
      "1980        8183      3550          WEST BENDIGO\n",
      "1981        8184      3550           WHITE HILLS\n",
      "\n",
      "[185 rows x 3 columns], 'chunk_3601':       Unnamed: 0  postcode       locality\n",
      "1982        8185      3551         ARNOLD\n",
      "1983        8186      3551    ARNOLD WEST\n",
      "1984        8187      3551          ASCOT\n",
      "1985        8188      3551      AXE CREEK\n",
      "1986        8189      3551        AXEDALE\n",
      "...          ...       ...            ...\n",
      "2205        8409      3597    LAKE POWELL\n",
      "2206        8410      3597        NARRUNG\n",
      "2207        8411      3597          NATYA\n",
      "2208        8412      3597        PIANGIL\n",
      "2209        8413      3599  BOUNDARY BEND\n",
      "\n",
      "[228 rows x 3 columns], 'chunk_3651':       Unnamed: 0  postcode         locality\n",
      "2210        8414      3607           TABILK\n",
      "2211        8415      3608       BAILIESTON\n",
      "2212        8416      3608    GOULBURN WEIR\n",
      "2213        8417      3608         GRAYTOWN\n",
      "2214        8418      3608   KIRWANS BRIDGE\n",
      "...          ...       ...              ...\n",
      "2336        8542      3646      YABBA SOUTH\n",
      "2337        8543      3646        YOUANMITE\n",
      "2338        8544      3647   DOOKIE COLLEGE\n",
      "2339        8545      3649       KATAMATITE\n",
      "2340        8546      3649  KATAMATITE EAST\n",
      "\n",
      "[131 rows x 3 columns], 'chunk_3701':       Unnamed: 0  postcode          locality\n",
      "2341        8547      3658         BROADFORD\n",
      "2342        8548      3658        CLONBINANE\n",
      "2343        8549      3658        CLONBINANE\n",
      "2344        8550      3658        FLOWERDALE\n",
      "2345        8551      3658         HAZELDENE\n",
      "...          ...       ...               ...\n",
      "2556        8763      3700           BULLIOH\n",
      "2557        8764      3700     GEORGES CREEK\n",
      "2558        8765      3700      JARVIS CREEK\n",
      "2559        8766      3700       TALLANGATTA\n",
      "2560        8767      3700  TALLANGATTA EAST\n",
      "\n",
      "[220 rows x 3 columns], 'chunk_3751':       Unnamed: 0  postcode      locality\n",
      "2561        8768      3701     DARTMOUTH\n",
      "2562        8769      3701       ESKDALE\n",
      "2563        8770      3701      FERNVALE\n",
      "2564        8771      3701  GRANITE FLAT\n",
      "2565        8772      3701        GRANYA\n",
      "...          ...       ...           ...\n",
      "2724        8932      3747      WOOLSHED\n",
      "2725        8933      3747      WOORAGEE\n",
      "2726        8934      3749      BRUARONG\n",
      "2727        8935      3749  YACKANDANDAH\n",
      "2728        8936      3750       WOLLERT\n",
      "\n",
      "[168 rows x 3 columns], 'chunk_3801':       Unnamed: 0  postcode           locality\n",
      "2729        8937      3751          WOODSTOCK\n",
      "2730        8938      3752       MORANG SOUTH\n",
      "2731        8939      3752       SOUTH MORANG\n",
      "2732        8940      3753          BEVERIDGE\n",
      "2733        8941      3754             DOREEN\n",
      "...          ...       ...                ...\n",
      "2816        9024      3799          MILLGROVE\n",
      "2817        9025      3799            REEFTON\n",
      "2818        9026      3799          WARBURTON\n",
      "2819        9027      3799            WESBURN\n",
      "2820        9028      3800  MONASH UNIVERSITY\n",
      "\n",
      "[92 rows x 3 columns], 'chunk_3851':       Unnamed: 0  postcode            locality\n",
      "2821        9029      3802     ENDEAVOUR HILLS\n",
      "2822        9030      3803              HALLAM\n",
      "2823        9031      3804   NARRE WARREN EAST\n",
      "2824        9032      3804  NARRE WARREN NORTH\n",
      "2825        9033      3805       FOUNTAIN GATE\n",
      "...          ...       ...                 ...\n",
      "2986        9194      3850             GIFFARD\n",
      "2987        9195      3850           GUTHRIDGE\n",
      "2988        9196      3850                SALE\n",
      "2989        9197      3850          SALE NORTH\n",
      "2990        9198      3850              WURRUK\n",
      "\n",
      "[170 rows x 3 columns], 'chunk_3901':       Unnamed: 0  postcode     locality\n",
      "2991        9199      3851        AIRLY\n",
      "2992        9200      3851  BUNDALAGUAH\n",
      "2993        9201      3851    CLYDEBANK\n",
      "2994        9202      3851      COBAINS\n",
      "2995        9203      3851     DARRIMAN\n",
      "...          ...       ...          ...\n",
      "3254        9462      3898  OMEO VALLEY\n",
      "3255        9463      3898  SHANNONVALE\n",
      "3256        9464      3900     BENAMBRA\n",
      "3257        9465      3900     COBBERAS\n",
      "3258        9466      3900      UPLANDS\n",
      "\n",
      "[268 rows x 3 columns], 'chunk_3951':       Unnamed: 0  postcode          locality\n",
      "3259        9467      3902         BUMBERRAH\n",
      "3260        9468      3902      JOHNSONVILLE\n",
      "3261        9469      3903        SWAN REACH\n",
      "3262        9470      3904            METUNG\n",
      "3263        9471      3909           KALIMNA\n",
      "...          ...       ...               ...\n",
      "3340        9548      3950    KARDELLA SOUTH\n",
      "3341        9549      3950        KORUMBURRA\n",
      "3342        9550      3950  KORUMBURRA SOUTH\n",
      "3343        9551      3950        STRZELECKI\n",
      "3344        9552      3950          WHITELAW\n",
      "\n",
      "[86 rows x 3 columns]}\n"
     ]
    }
   ],
   "source": [
    "print(chunk_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run cell above\n",
    "2. Run cell below \n",
    "3. Run cell below the cell below\n",
    "4. Run property_metadata.write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working METHOD\n",
    "import re\n",
    "from json import dump\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.master('local[*]') \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .appName(\"PropertyScraper\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://www.domain.com.au\"\n",
    "N_PAGES = range(1, 50)  # Max number of pages you want to scrape  \n",
    "\n",
    "# Load suburbs CSV\n",
    "suburbs_df = pd.read_csv('postcodes.csv')  # Ensure this CSV contains 'suburb' and 'postcode' columns\n",
    "chunk_dict = get_chunks(suburbs_df)\n",
    "\n",
    "def start_scrape(chunk, file_suffix):\n",
    "    \"\"\"Function that scrapes https://www.domain.com.au and outputs the data into a JSON file\n",
    "    \n",
    "    parameters:\n",
    "    chunk: chunk of 50 postcodes we will scrape\n",
    "    file_suffix: what we want to title the end of our files when we write to json\n",
    "    \"\"\"\n",
    "\n",
    "    # Define schema for the Spark DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"url\", StringType(), True),\n",
    "        StructField(\"postcode\", StringType(), True),\n",
    "        StructField(\"suburb\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"cost_text\", StringType(), True),\n",
    "        StructField(\"beds\", StringType(), True),  # Separate field for beds\n",
    "        StructField(\"baths\", StringType(), True),  # Separate field for baths\n",
    "        StructField(\"parking\", StringType(), True),  # Parking field\n",
    "        StructField(\"property_type\", StringType(), True),  # Property type field\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Initialize an empty DataFrame with the schema\n",
    "    property_metadata = spark.createDataFrame([], schema)\n",
    "\n",
    "    # Loop through each suburb and its postcode\n",
    "    for index, row in chunk.iterrows():\n",
    "        suburb = row['locality'].lower().replace(' ', '-')  # Convert to lowercase and hyphenate\n",
    "        postcode = row['postcode']\n",
    "\n",
    "        print(f\"Scraping data for {suburb} ({postcode})\")\n",
    "\n",
    "        url_links = []\n",
    "        page_found = False  # This flag will help us track whether any results are found\n",
    "\n",
    "        # Generate list of URLs to visit\n",
    "        for page in N_PAGES:\n",
    "            url = BASE_URL + f\"/rent/{suburb}-vic-{postcode}/?ssubs=0&sort=suburb-asc&page={page}\"\n",
    "            try:\n",
    "                bs_object = BeautifulSoup(urlopen(Request(url, headers={'User-Agent': \"PostmanRuntime/7.6.0\"})), \"lxml\")\n",
    "\n",
    "                # Check if the page has results or shows \"No results found\"\n",
    "                no_results = bs_object.find(text=re.compile(\"No results found\", re.I))\n",
    "                if no_results:\n",
    "                    print(f\"No results found for {suburb} on page {page}. Stopping further scraping for this suburb.\")\n",
    "                    break  # Exit the pagination loop if no results are found\n",
    "\n",
    "                # Find property links\n",
    "                index_links = bs_object.find(\"ul\", {\"data-testid\": \"results\"})\n",
    "                if not index_links:\n",
    "                    print(f\"No more results for {suburb} on page {page}.\")\n",
    "                    break  # Exit pagination if no results list is found (end of pages)\n",
    "\n",
    "                index_links = index_links.findAll(\"a\", href=re.compile(f\"{BASE_URL}/*\"))\n",
    "                page_found = True  # At least one result was found on this page\n",
    "\n",
    "                for link in index_links:\n",
    "                    # If it's a property address, add it to the list\n",
    "                    if 'address' in link.get('class', []):\n",
    "                        url_links.append(link['href'])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {url}: {e}\")\n",
    "                break  # Stop if there's an issue with fetching the page\n",
    "\n",
    "        if not page_found:\n",
    "            print(f\"No results for {suburb}. Moving to the next suburb.\")\n",
    "            continue  # Skip to the next suburb if no pages were found for this one\n",
    "\n",
    "        # For each URL, scrape some basic metadata\n",
    "        pbar = tqdm(url_links)\n",
    "        success_count, total_count = 0, 0\n",
    "\n",
    "        for property_url in pbar:\n",
    "            try:\n",
    "                bs_object = BeautifulSoup(urlopen(Request(property_url, headers={'User-Agent': \"PostmanRuntime/7.6.0\"})), \"lxml\")\n",
    "                total_count += 1\n",
    "\n",
    "                # Get property name\n",
    "                name = bs_object.find(\"h1\", {\"class\": \"css-164r41r\"}).text.strip()\n",
    "\n",
    "                # Get cost text\n",
    "                cost_text = bs_object.find(\"div\", {\"data-testid\": \"listing-details__summary-title\"}).text.strip()\n",
    "\n",
    "                # Get rooms (beds and baths)\n",
    "                rooms = bs_object.find(\"div\", {\"data-testid\": \"property-features\"}).findAll(\n",
    "                    \"span\", {\"data-testid\": \"property-features-text-container\"}\n",
    "                )\n",
    "\n",
    "                # Initialize variables\n",
    "                beds, baths, parking = None, None, '0'  # Default value for parking is '0 Car'\n",
    "\n",
    "                for feature in rooms:\n",
    "                    text = feature.text\n",
    "                    if 'Bed' in text:\n",
    "                        beds_match = re.findall(r'\\d+', text)\n",
    "                        if beds_match:\n",
    "                            beds = beds_match[0]  # Extract the number of beds\n",
    "                    elif 'Bath' in text:\n",
    "                        baths_match = re.findall(r'\\d+', text)\n",
    "                        if baths_match:\n",
    "                            baths = baths_match[0]  # Extract the number of baths\n",
    "                    elif 'Car' in text or 'Parking' in text:\n",
    "                        parking_match = re.findall(r'\\d+', text)\n",
    "                        if parking_match:\n",
    "                            parking = parking_match[0]  # Extract the number of parking spaces\n",
    "\n",
    "                property_type_container = bs_object.find(\"div\", {\"data-testid\": \"listing-summary-property-type\"})\n",
    "                property_type = property_type_container.get_text(strip=True)\n",
    "\n",
    "                # Create a row and append it to the DataFrame\n",
    "                row = [(property_url, postcode, suburb, name, cost_text, beds, baths, parking, property_type)]\n",
    "                row_df = spark.createDataFrame(row, schema)\n",
    "                property_metadata = property_metadata.union(row_df)\n",
    "                success_count += 1\n",
    "\n",
    "            except AttributeError:\n",
    "                print(f\"Error scraping {property_url}: missing data\")\n",
    "\n",
    "            pbar.set_description(f\"{(success_count / total_count * 100):.0f}% successful\")\n",
    "\n",
    "        # Show the DataFrame to ensure data is being appended\n",
    "        #property_metadata.show()\n",
    "\n",
    "    # Output to parquet file\n",
    "    #try:\n",
    "     #   property_metadata.write.mode(\"overwrite\").json('../data/raw/work_{}.json'.format(file_suffix))\n",
    "      #  print(f\"Data successfully written\")\n",
    "    #except Exception as e:\n",
    "     #   print(f\"An error occured: {e}\")\n",
    "\n",
    "    #added this print statement so that the cell output can be scrollable - it's getting annoying to click the scroll bar >:(\n",
    "    print(\"chunk finished\")\n",
    "    return property_metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'chunk_3000'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# start scraping by chunks of 50\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#for i in chunk_dict:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m  \u001b[38;5;66;03m#   start_scrape(chunk_dict[i], i.split(\"_\")[1])\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m property_metadata \u001b[38;5;241m=\u001b[39m start_scrape(\u001b[43mchunk_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchunk_3000\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3000\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m## changed 3050 , 3050\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'chunk_3000'"
     ]
    }
   ],
   "source": [
    "# start scraping by chunks of 50\n",
    "#for i in chunk_dict:\n",
    " #   start_scrape(chunk_dict[i], i.split(\"_\")[1])\n",
    "property_metadata = start_scrape(chunk_dict['chunk_3000'], '3000')  ## changed 3050 , 3050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+--------------------+-------------+----+-----+-------+--------------------+\n",
      "|                 url|postcode|   suburb|                name|    cost_text|beds|baths|parking|       property_type|\n",
      "+--------------------+--------+---------+--------------------+-------------+----+-----+-------+--------------------+\n",
      "|https://www.domai...|    3000|melbourne|3113/639 Lonsdale...|    $1,200.00|   3|    2|      1|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|1503/270 King Str...|$850 Per Week|   4|    1|      1|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|403/639 Lonsdale ...| $750per week|   2|    2|      0|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|3409/138 Spencer ...|      $625 pw|   1|    1|      0|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|103/300 Swanston ...|      $620 pw|   1|    1|      1|Apartment / Unit ...|\n",
      "+--------------------+--------+---------+--------------------+-------------+----+-----+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "property_metadata.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "property_metadata.write.mode(\"overwrite\").parquet(\"../data/raw/work_3050.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.parquet(\"../data/raw/work_3050.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+--------------------+--------------------+----+-----+-------+--------------------+\n",
      "|                 url|postcode|   suburb|                name|           cost_text|beds|baths|parking|       property_type|\n",
      "+--------------------+--------+---------+--------------------+--------------------+----+-----+-------+--------------------+\n",
      "|https://www.domai...|    3000|melbourne|99 Franklin Stree...|Furnished, bills,...|   1|    1|      0|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|103/19 Exploratio...|$540 Per Week Inc...|   1|    1|      0|              Studio|\n",
      "|https://www.domai...|    3000|melbourne|236 La Trobe Stre...|Furnished, all in...|   1|    1|      0|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|406/10 Bennetts L...|$590 PW Fully Fur...|   1|    1|      0|              Studio|\n",
      "|https://www.domai...|    3000|melbourne|2313/250 Elizabet...|$620 per week fur...|   1|    1|      0|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|707 10 Bennetts L...|$630 Bills and Wi...|   1|    1|      0|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|908/557 Little Lo...|          $475.00 pw|   1|    1|      0|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|509/318 Little Bo...|       $490 per week|   1|    1|      0|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|505/639 Little Bo...|       $550 per week|   2|    1|      0|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|1403/318 Little L...|             $795.00|   2|    2|      1|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|703/19 Exploratio...|$580 Per Week Inc...|   1|    1|      0|              Studio|\n",
      "|https://www.domai...|    3000|melbourne|38/562 Little Bou...|       $550 per week|   1|    2|      0|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|6901/462 Elizabet...|$800/w PARTLY-FUR...|   2|    2|      0|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|104/538 Little Lo...|             $575 pw|   2|    1|      1|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|208/601 Little Co...|             $625 pw|   2|    2|      0|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|1310/442 Elizabet...|       $650 per week|   2|    1|      0|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|7306/462 Elizabet...|       $1,250 weekly|   3|    2|      1|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|3.5 & 6/201 Sprin...|       $670 per week|   2|    2|      1|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|1903/639 Lonsdale...|       $600 per week|   1|    1|      0|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|1407/555 Flinders...|       $725 per week|   2|    1|      1|Apartment / Unit ...|\n",
      "+--------------------+--------+---------+--------------------+--------------------+----+-----+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = suburbs_df[suburbs_df['postcode'] >= 3950]\n",
    "chunk_dict['chunk_3997'] = temp[temp['postcode'] < 3997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mchunk_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[1;32m      2\u001b[0m     start_scrape(chunk_dict[i])\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "for i in chunk_dict[1:]:\n",
    "    start_scrape(chunk_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/home/priscillapei/project-2-group-real-estate-industry-project-22/data/raw/work_3050.json.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m      2\u001b[0m         StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      3\u001b[0m         StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostcode\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperty_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),  \u001b[38;5;66;03m# Property type field\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     ])\n\u001b[0;32m---> 12\u001b[0m work \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/raw/work_3050.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:425\u001b[0m, in \u001b[0;36mDataFrameReader.json\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator: Iterable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/home/priscillapei/project-2-group-real-estate-industry-project-22/data/raw/work_3050.json."
     ]
    }
   ],
   "source": [
    "\n",
    "schema = StructType([\n",
    "        StructField(\"url\", StringType(), True),\n",
    "        StructField(\"postcode\", StringType(), True),\n",
    "        StructField(\"suburb\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"cost_text\", StringType(), True),\n",
    "        StructField(\"beds\", StringType(), True),  # Separate field for beds\n",
    "        StructField(\"baths\", StringType(), True),  # Separate field for baths\n",
    "        StructField(\"parking\", StringType(), True),  # Parking field\n",
    "        StructField(\"property_type\", StringType(), True),  # Property type field\n",
    "    ])\n",
    "work = spark.read.schema(schema).json('../data/raw/work_3050.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+----+---------+----+-----+-------+-------------+\n",
      "|url|postcode|suburb|name|cost_text|beds|baths|parking|property_type|\n",
      "+---+--------+------+----+---------+----+-----+-------+-------------+\n",
      "+---+--------+------+----+---------+----+-----+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "work.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  url  postcode   suburb  \\\n",
      "0   https://www.domain.com.au/34-evadene-drive-tar...      3029  tarneit   \n",
      "1   https://www.domain.com.au/434-bethany-road-tar...      3029  tarneit   \n",
      "2   https://www.domain.com.au/58-antonio-road-tarn...      3029  tarneit   \n",
      "3   https://www.domain.com.au/12-lindeman-street-t...      3029  tarneit   \n",
      "4   https://www.domain.com.au/3-imatra-loop-tarnei...      3029  tarneit   \n",
      "5   https://www.domain.com.au/84-lucania-crescent-...      3029  tarneit   \n",
      "6   https://www.domain.com.au/8-keeping-terrace-ta...      3029  tarneit   \n",
      "7   https://www.domain.com.au/40-kamala-drive-tarn...      3029  tarneit   \n",
      "8   https://www.domain.com.au/48-riland-boulevard-...      3029  tarneit   \n",
      "9   https://www.domain.com.au/9-ceremony-drive-tar...      3029  tarneit   \n",
      "10  https://www.domain.com.au/11-ogawa-walk-tarnei...      3029  tarneit   \n",
      "11  https://www.domain.com.au/tarneit-vic-3029-151...      3029  tarneit   \n",
      "\n",
      "                                     name                 cost_text  beds  \\\n",
      "0      34 Evadene Drive, Tarneit VIC 3029             $620 per week     4   \n",
      "1      434 Bethany Road, Tarneit VIC 3029                      $520     3   \n",
      "2       58 Antonio Road, Tarneit VIC 3029             $580 Per Week     4   \n",
      "3    12 Lindeman Street, Tarneit VIC 3029                   $650 pw     4   \n",
      "4         3 Imatra Loop, Tarneit VIC 3029                      $570     4   \n",
      "5   84 Lucania Crescent, Tarneit VIC 3029             Contact Agent     4   \n",
      "6     8 Keeping Terrace, Tarneit VIC 3029            $ 500 PER WEEK     3   \n",
      "7       40 Kamala Drive, Tarneit VIC 3029                      $520     3   \n",
      "8   48 Riland Boulevard, Tarneit VIC 3029                      $520     3   \n",
      "9      9 Ceremony Drive, Tarneit VIC 3029                       610     4   \n",
      "10        11 Ogawa Walk, Tarneit VIC 3029                       530     3   \n",
      "11                       Tarneit VIC 3029  Rent2own with No Deposit     4   \n",
      "\n",
      "    baths  parking property_type  \n",
      "0       3        2         House  \n",
      "1       2        2         House  \n",
      "2       2        2         House  \n",
      "3       2        2         House  \n",
      "4       2        2         House  \n",
      "5       2        2         House  \n",
      "6       2        1         House  \n",
      "7       2        1         House  \n",
      "8       2        1         House  \n",
      "9       2        2         House  \n",
      "10      2        2         House  \n",
      "11      2        2         House  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the folder path containing JSON files\n",
    "folder_path = '../data/raw/work.json'\n",
    "\n",
    "# List all files in the directory\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Read each JSON file into a DataFrame\n",
    "for file in json_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    # Read JSON file\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "    # Append the DataFrame to the list\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(combined_df.head(12))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
