{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/14 16:42:27 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "## METHOD 1: convert dictionary to spark dataframe and append to initialized sdf\n",
    "# built-in imports\n",
    "import re\n",
    "from json import dump, load\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "import pandas as pd  \n",
    "import os\n",
    "# Import Spark modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Domain Scraper\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#### create a spark data frame\n",
    "\n",
    "# constants\n",
    "BASE_URL = \"https://www.domain.com.au\"\n",
    "N_PAGES = range(1, 2)  # Update this to your liking\n",
    "\n",
    "#Scrape suburb from the address\n",
    "def extract_suburb(address: str) -> str:\n",
    "    \"\"\"Extract the suburb name from the property address.\"\"\"\n",
    "    match = re.search(r'(?<=, )\\w+', address)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def start_scrape() -> None:\n",
    "    \"\"\" Function that scrapes https://www.domain.com.au and outputs the data into a json file\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    schema = StructType([\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"cost_text\", StringType(), True),\n",
    "    StructField(\"rooms\", StringType(), True),\n",
    "    StructField(\"desc\", StringType(), True),\n",
    "    StructField(\"parking\", StringType(), True),\n",
    "    StructField(\"street\", StringType(), True),\n",
    "    StructField(\"suburb\", StringType(), True),\n",
    "    StructField(\"postcode\", StringType(), True),\n",
    "    StructField(\"propertyType\", StringType(), True),\n",
    "    StructField(\"school\", StringType(), True),\n",
    "    StructField(\"features\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "    # begin code\n",
    "    url_links = []\n",
    "    property_metadata = defaultdict(dict)\n",
    "    sdf = spark.createDataFrame([],schema)\n",
    "    \n",
    "\n",
    "    # generate list of urls to visit\n",
    "    for page in N_PAGES:\n",
    "        url = BASE_URL + f\"/rent/?excludedeposittaken=1&state=vic&page={page}\"\n",
    "        print(f\"Visiting {url}\")\n",
    "        bs_object = BeautifulSoup(urlopen(Request(url, headers={'User-Agent': \"PostmanRuntime/7.6.0\"})), \"lxml\")\n",
    "\n",
    "        # find the unordered list (ul) elements which are the results, then\n",
    "        # find all href (a) tags that are from the base_url website.\n",
    "        index_links = bs_object.find(\"ul\", {\"data-testid\": \"results\"}).findAll(\n",
    "            \"a\", href=re.compile(f\"{BASE_URL}/*\")  # the `*` denotes wildcard any\n",
    "        )\n",
    "\n",
    "        for link in index_links:\n",
    "            # if it's a property address, add it to the list\n",
    "            if 'address' in link.get('class', []):\n",
    "                url_links.append(link['href'])\n",
    "\n",
    "    # for each url, scrape some basic metadata\n",
    "    pbar = tqdm(url_links)\n",
    "    success_count, total_count = 0, 0\n",
    "\n",
    "    for property_url in pbar:\n",
    "        try:\n",
    "            bs_object = BeautifulSoup(urlopen(Request(property_url, headers={'User-Agent': \"PostmanRuntime/7.6.0\"})), \"lxml\")\n",
    "            total_count += 1\n",
    "\n",
    "            property_page = urlopen(Request(property_url, headers={'User-Agent': \"PostmanRuntime/7.6.0\"}))\n",
    "            property_soup = BeautifulSoup(property_page, \"lxml\")\n",
    "\n",
    "            # looks for the header class to get property name\n",
    "            property_metadata[property_url]['name'] = bs_object.find(\"h1\", {\"class\": \"css-164r41r\"}).text.strip()\n",
    "\n",
    "            # looks for the div containing a summary title for cost\n",
    "            property_metadata[property_url]['cost_text'] = bs_object.find(\n",
    "                \"div\", {\"data-testid\": \"listing-details__summary-title\"}\n",
    "            ).text.strip()\n",
    "\n",
    "\n",
    "            # get rooms and parking\n",
    "            rooms = bs_object.find(\"div\", {\"data-testid\": \"property-features\"}).findAll(\n",
    "                \"span\", {\"data-testid\": \"property-features-text-container\"}\n",
    "            )\n",
    "\n",
    "            # rooms\n",
    "            property_metadata[property_url]['rooms'] = \", \".join(\n",
    "                [re.findall(r'\\d+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'Bed' in feature.text or 'Bath' in feature.text]\n",
    "            )\n",
    "\n",
    "            # parking\n",
    "            property_metadata[property_url]['parking'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'Parking' in feature.text]\n",
    "            )\n",
    "\n",
    "            # desc\n",
    "            property_metadata[property_url]['desc'] = \", \".join(\n",
    "                [re.findall(r'\\d+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'desc' in feature.text]\n",
    "            )\n",
    "            \n",
    "            # listingID:\n",
    "            property_metadata[property_url]['listingid'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'listingId' in feature.text]\n",
    "            )\n",
    "\n",
    "            # street:\n",
    "            property_metadata[property_url]['street'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'street' in feature.text]\n",
    "            )\n",
    "\n",
    "            # suburb:\n",
    "            property_metadata[property_url]['suburb'] = extract_suburb(property_metadata[property_url]['name'])\n",
    "\n",
    "            \n",
    "            # postcode:\n",
    "            property_metadata[property_url]['postcode'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'postcode' in feature.text]\n",
    "            )\n",
    "\n",
    "            # property type:\n",
    "            property_metadata[property_url]['propertyType'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'apartment' in feature.text \n",
    "                 or 'unit' in feature.text or 'house' in feature.text or 'flat' in feature.text]\n",
    "            )\n",
    "\n",
    "            # schools:\n",
    "            property_metadata[property_url]['school'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'school' in feature.text]\n",
    "            )\n",
    "\n",
    "            # features:\n",
    "            property_metadata[property_url]['features'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'feature' in feature.text]\n",
    "            )\n",
    "\n",
    "            # loanfinder:\n",
    "            property_metadata[property_url]['loan'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'loan' in feature.text]\n",
    "            )\n",
    "\n",
    "            # listingSummary:\n",
    "            property_metadata[property_url]['listingsummary'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'summary' in feature.text]\n",
    "            )\n",
    "\n",
    "            # suburb insights:\n",
    "            property_metadata[property_url]['suburbInsights'] = \", \".join(\n",
    "                [re.findall(r'\\S+\\s[A-Za-z]+', feature.text)[0] for feature in rooms if 'suburbInsights' in feature.text]\n",
    "            )\n",
    "\n",
    "            # property description\n",
    "            property_metadata[property_url]['desc'] = bs_object.find(\"p\").text.strip() if bs_object.find(\"p\") else \"N/A\"\n",
    "\n",
    "\n",
    "            # Scrape property description\n",
    "            property_metadata[property_url]['desc'] = re.sub(r'<br\\/>', '\\n', str(property_soup.find(\"p\"))).strip('</p>')\n",
    "           \n",
    "            \"\"\"\n",
    "            # Write each row to the CSV\n",
    "            writer.writerow([\n",
    "                property_url,\n",
    "                property_metadata[property_url]['name'],\n",
    "                property_metadata[property_url]['cost_text'],\n",
    "                property_metadata[property_url]['rooms'],\n",
    "                property_metadata[property_url]['parking'],\n",
    "                property_metadata[property_url]['desc'],\n",
    "                property_metadata[property_url]['listingid'],\n",
    "                property_metadata[property_url]['street'],\n",
    "                property_metadata[property_url]['suburb'],\n",
    "                property_metadata[property_url]['postcode'],\n",
    "                property_metadata[property_url]['propertyType'],\n",
    "                property_metadata[property_url]['school'],\n",
    "                property_metadata[property_url]['features'],\n",
    "                property_metadata[property_url]['loan'],\n",
    "                property_metadata[property_url]['listingsummary'],\n",
    "                property_metadata[property_url]['suburbInsights']\n",
    "            ])\n",
    "            \"\"\"\n",
    "            success_count += 1\n",
    "            temp_sdf = spark.createDataFrame(property_metadata)\n",
    "            sdf.union(temp_sdf)\n",
    "\n",
    "        except AttributeError:\n",
    "            print(f\"Issue with {property_url}\")\n",
    "\n",
    "        pbar.set_description(f\"{(success_count / total_count * 100):.0f}% successful\")\n",
    "\n",
    "        # output to example json in data/raw/\n",
    "    with open('../data/raw/example.json', 'w') as f:\n",
    "        dump(property_metadata, f)\n",
    "\n",
    "def convert_to_parquet(filepath: str, output_path: str) -> None:\n",
    "    \"\"\" Function converts a json file into a parquet file\n",
    "\n",
    "    Parameters:\n",
    "    filepath (str): the filepath that locates our json data\n",
    "\n",
    "    output_path (str): the filepath that we will place our new parquet file into\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    with open(filepath) as f:\n",
    "        data = load(f)\n",
    "\n",
    "    new_data = change_json_format(data)\n",
    "\n",
    "    # conversion from json -> dataframe -> parquet\n",
    "    df = pd.DataFrame(new_data)\n",
    "    df.to_parquet(output_path, engine='pyarrow')\n",
    "\n",
    "    delete_json_file(filepath)\n",
    "\n",
    "# function that changes the formatting of the json file\n",
    "def change_json_format(data: dict) -> dict:\n",
    "    \"\"\" Function grabs the renames the json keys to the words after the last backslash in the url and adds the url as an item\n",
    "\n",
    "    Parameters:\n",
    "    data (dict): json dictionary we are changing\n",
    "\n",
    "    Returns:\n",
    "    dict: our new json dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    new_data = {}\n",
    "    for i in data.keys():\n",
    "        new_name = i.rsplit('/', 1)[-1]\n",
    "        new_data[new_name] = data[i]\n",
    "        new_data[new_name][\"href\"] = i\n",
    "    return new_data\n",
    "\n",
    "def delete_json_file(filepath: str) -> None:\n",
    "    \"\"\" Function deletes the json file we are converting from\n",
    "\n",
    "    Parameters:\n",
    "    filepath (string): filepath to the json file we are deleting\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.remove(filepath)\n",
    "        print(f\"File '{filepath}' deleted successfully\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{filepath}' not found\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: '{filepath}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/14 17:12:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "from pyarrow import json\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_parquet(filepath: str, output_path: str) -> None:\n",
    "    \"\"\" Function converts a JSON file into a parquet file \"\"\"\n",
    "    with open(filepath) as f:\n",
    "        data = load(f)\n",
    "\n",
    "    new_data = change_json_format(data)\n",
    "\n",
    "    # Conversion from JSON -> DataFrame -> Parquet\n",
    "    df = pd.DataFrame(new_data)\n",
    "    df.to_parquet(output_path, engine='pyarrow')\n",
    "\n",
    "    delete_json_file(filepath)\n",
    "\n",
    "def change_json_format(data: dict) -> dict:\n",
    "    \"\"\" Function renames JSON keys and adds the URL as an item \"\"\"\n",
    "    new_data = {}\n",
    "    for i in data.keys():\n",
    "        new_name = i.rsplit('/', 1)[-1]\n",
    "        new_data[new_name] = data[i]\n",
    "        new_data[new_name][\"href\"] = i\n",
    "    return new_data\n",
    "\n",
    "def delete_json_file(filepath: str) -> None:\n",
    "    \"\"\" Function deletes the JSON file \"\"\"\n",
    "    try:\n",
    "        os.remove(filepath)\n",
    "        print(f\"File '{filepath}' deleted successfully\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{filepath}' not found\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: '{filepath}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "def get_chunks(suburbs_df) -> dict:\n",
    "    \"\"\"function that splits up postcodes into chunks of 50 so that if we are kicked halfway during scraping we don't lose too much progress\n",
    "    \"\"\"\n",
    "    chunk_dict = {}\n",
    "    \n",
    "    i = 3048\n",
    "    j = 3023  \n",
    "    while i < 3997:\n",
    "        temp = suburbs_df[suburbs_df['postcode'] >= j]\n",
    "        chunk_dict['chunk_{}'.format(i)] = temp[temp['postcode'] <= i]\n",
    "        j += 25\n",
    "        i += 25\n",
    "\n",
    "    return chunk_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_3048\n",
      "chunk_3073\n",
      "chunk_3098\n",
      "chunk_3123\n",
      "chunk_3148\n",
      "chunk_3173\n",
      "chunk_3198\n",
      "chunk_3223\n",
      "chunk_3248\n",
      "chunk_3273\n",
      "chunk_3298\n",
      "chunk_3323\n",
      "chunk_3348\n",
      "chunk_3373\n",
      "chunk_3398\n",
      "chunk_3423\n",
      "chunk_3448\n",
      "chunk_3473\n",
      "chunk_3498\n",
      "chunk_3523\n",
      "chunk_3548\n",
      "chunk_3573\n",
      "chunk_3598\n",
      "chunk_3623\n",
      "chunk_3648\n",
      "chunk_3673\n",
      "chunk_3698\n",
      "chunk_3723\n",
      "chunk_3748\n",
      "chunk_3773\n",
      "chunk_3798\n",
      "chunk_3823\n",
      "chunk_3848\n",
      "chunk_3873\n",
      "chunk_3898\n",
      "chunk_3923\n",
      "chunk_3948\n",
      "chunk_3973\n"
     ]
    }
   ],
   "source": [
    "chunk_dict = get_chunks(suburbs_df)\n",
    "for i in chunk_dict:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run cell above\n",
    "2. Run cell below \n",
    "3. Run cell below the cell below\n",
    "4. Run property_metadata.write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working METHOD\n",
    "import re\n",
    "from json import dump\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.master('local[*]') \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .appName(\"PropertyScraper\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://www.domain.com.au\"\n",
    "N_PAGES = range(1, 50)  # Max number of pages you want to scrape  \n",
    "\n",
    "# Load suburbs CSV\n",
    "suburbs_df = pd.read_csv('postcodes.csv')  # Ensure this CSV contains 'suburb' and 'postcode' columns\n",
    "chunk_dict = get_chunks(suburbs_df)\n",
    "\n",
    "def start_scrape(chunk, file_suffix):\n",
    "    \"\"\"Function that scrapes https://www.domain.com.au and outputs the data into a JSON file\n",
    "    \n",
    "    parameters:\n",
    "    chunk: chunk of 50 postcodes we will scrape\n",
    "    file_suffix: what we want to title the end of our files when we write to json\n",
    "    \"\"\"\n",
    "\n",
    "    # Define schema for the Spark DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"url\", StringType(), True),\n",
    "        StructField(\"postcode\", StringType(), True),\n",
    "        StructField(\"suburb\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"cost_text\", StringType(), True),\n",
    "        StructField(\"beds\", StringType(), True),  # Separate field for beds\n",
    "        StructField(\"baths\", StringType(), True),  # Separate field for baths\n",
    "        StructField(\"parking\", StringType(), True),  # Parking field\n",
    "        StructField(\"property_type\", StringType(), True),  # Property type field\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Initialize an empty DataFrame with the schema\n",
    "    property_metadata = spark.createDataFrame([], schema)\n",
    "\n",
    "    # Loop through each suburb and its postcode\n",
    "    for index, row in chunk.iterrows():\n",
    "        suburb = row['locality'].lower().replace(' ', '-')  # Convert to lowercase and hyphenate\n",
    "        postcode = row['postcode']\n",
    "\n",
    "        print(f\"Scraping data for {suburb} ({postcode})\")\n",
    "\n",
    "        url_links = []\n",
    "        page_found = False  # This flag will help us track whether any results are found\n",
    "\n",
    "        # Generate list of URLs to visit\n",
    "        for page in N_PAGES:\n",
    "            url = BASE_URL + f\"/rent/{suburb}-vic-{postcode}/?ssubs=0&sort=suburb-asc&page={page}\"\n",
    "            try:\n",
    "                bs_object = BeautifulSoup(urlopen(Request(url, headers={'User-Agent': \"PostmanRuntime/7.6.0\"})), \"lxml\")\n",
    "\n",
    "                # Check if the page has results or shows \"No results found\"\n",
    "                no_results = bs_object.find(text=re.compile(\"No results found\", re.I))\n",
    "                if no_results:\n",
    "                    print(f\"No results found for {suburb} on page {page}. Stopping further scraping for this suburb.\")\n",
    "                    break  # Exit the pagination loop if no results are found\n",
    "\n",
    "                # Find property links\n",
    "                index_links = bs_object.find(\"ul\", {\"data-testid\": \"results\"})\n",
    "                if not index_links:\n",
    "                    print(f\"No more results for {suburb} on page {page}.\")\n",
    "                    break  # Exit pagination if no results list is found (end of pages)\n",
    "\n",
    "                index_links = index_links.findAll(\"a\", href=re.compile(f\"{BASE_URL}/*\"))\n",
    "                page_found = True  # At least one result was found on this page\n",
    "\n",
    "                for link in index_links:\n",
    "                    # If it's a property address, add it to the list\n",
    "                    if 'address' in link.get('class', []):\n",
    "                        url_links.append(link['href'])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {url}: {e}\")\n",
    "                break  # Stop if there's an issue with fetching the page\n",
    "\n",
    "        if not page_found:\n",
    "            print(f\"No results for {suburb}. Moving to the next suburb.\")\n",
    "            continue  # Skip to the next suburb if no pages were found for this one\n",
    "\n",
    "        # For each URL, scrape some basic metadata\n",
    "        pbar = tqdm(url_links)\n",
    "        success_count, total_count = 0, 0\n",
    "\n",
    "        for property_url in pbar:\n",
    "            try:\n",
    "                bs_object = BeautifulSoup(urlopen(Request(property_url, headers={'User-Agent': \"PostmanRuntime/7.6.0\"})), \"lxml\")\n",
    "                total_count += 1\n",
    "\n",
    "                # Get property name\n",
    "                name = bs_object.find(\"h1\", {\"class\": \"css-164r41r\"}).text.strip()\n",
    "\n",
    "                # Get cost text\n",
    "                cost_text = bs_object.find(\"div\", {\"data-testid\": \"listing-details__summary-title\"}).text.strip()\n",
    "\n",
    "                # Get rooms (beds and baths)\n",
    "                rooms = bs_object.find(\"div\", {\"data-testid\": \"property-features\"}).findAll(\n",
    "                    \"span\", {\"data-testid\": \"property-features-text-container\"}\n",
    "                )\n",
    "\n",
    "                # Initialize variables\n",
    "                beds, baths, parking = None, None, '0'  # Default value for parking is '0 Car'\n",
    "\n",
    "                for feature in rooms:\n",
    "                    text = feature.text\n",
    "                    if 'Bed' in text:\n",
    "                        beds_match = re.findall(r'\\d+', text)\n",
    "                        if beds_match:\n",
    "                            beds = beds_match[0]  # Extract the number of beds\n",
    "                    elif 'Bath' in text:\n",
    "                        baths_match = re.findall(r'\\d+', text)\n",
    "                        if baths_match:\n",
    "                            baths = baths_match[0]  # Extract the number of baths\n",
    "                    elif 'Car' in text or 'Parking' in text:\n",
    "                        parking_match = re.findall(r'\\d+', text)\n",
    "                        if parking_match:\n",
    "                            parking = parking_match[0]  # Extract the number of parking spaces\n",
    "\n",
    "                property_type_container = bs_object.find(\"div\", {\"data-testid\": \"listing-summary-property-type\"})\n",
    "                property_type = property_type_container.get_text(strip=True)\n",
    "\n",
    "                # Create a row and append it to the DataFrame\n",
    "                row = [(property_url, postcode, suburb, name, cost_text, beds, baths, parking, property_type)]\n",
    "                row_df = spark.createDataFrame(row, schema)\n",
    "                property_metadata = property_metadata.union(row_df)\n",
    "                success_count += 1\n",
    "\n",
    "            except AttributeError:\n",
    "                print(f\"Error scraping {property_url}: missing data\")\n",
    "\n",
    "            pbar.set_description(f\"{(success_count / total_count * 100):.0f}% successful\")\n",
    "\n",
    "        # Show the DataFrame to ensure data is being appended\n",
    "        #property_metadata.show()\n",
    "\n",
    "    # Output to parquet file\n",
    "    try:\n",
    "        property_metadata.write.mode(\"overwrite\").parquet('../data/raw/work_{}.parquet'.format(file_suffix))\n",
    "        print(f\"Data successfully written\")\n",
    "    except Exception as e:\n",
    "       print(f\"An error occured: {e}\")\n",
    "\n",
    "    #added this print statement so that the cell output can be scrollable - it's getting annoying to click the scroll bar >:(\n",
    "    print(\"chunk finished\")\n",
    "    #return property_metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for melbourne (3000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sx/x0l__4fj6r19x5mnz6dwh8300000gn/T/ipykernel_40073/1616537980.py:69: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  no_results = bs_object.find(text=re.compile(\"No results found\", re.I))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more results for melbourne on page 45.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 869/869 [08:26<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# start scraping by chunks of 50\n",
    "#for i in chunk_dict:\n",
    " #   start_scrape(chunk_dict[i], i.split(\"_\")[1])\n",
    "property_metadata = start_scrape(chunk_dict['chunk_3000'], '3000')  ## changed 3050 , 3050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/14 17:28:53 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "24/09/14 17:28:55 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "24/09/14 17:28:55 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "24/09/14 17:28:57 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+--------------------+-------------+----+-----+-------+--------------------+\n",
      "|                 url|postcode|   suburb|                name|    cost_text|beds|baths|parking|       property_type|\n",
      "+--------------------+--------+---------+--------------------+-------------+----+-----+-------+--------------------+\n",
      "|https://www.domai...|    3000|melbourne|3113/639 Lonsdale...|    $1,200.00|   3|    2|      1|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|1503/270 King Str...|$850 Per Week|   4|    1|      1|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|403/639 Lonsdale ...| $750per week|   2|    2|      0|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|3409/138 Spencer ...|      $625 pw|   1|    1|      0|Apartment / Unit ...|\n",
      "|https://www.domai...|    3000|melbourne|103/300 Swanston ...|      $620 pw|   1|    1|      1|Apartment / Unit ...|\n",
      "+--------------------+--------+---------+--------------------+-------------+----+-----+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "property_metadata.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/14 17:29:53 WARN DAGScheduler: Broadcasting large task binary with size 5.5 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "property_metadata.write.mode(\"overwrite\").parquet(\"../data/raw/work_3000.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.parquet(\"../data/raw/work_3000.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>url</th><th>postcode</th><th>suburb</th><th>name</th><th>cost_text</th><th>beds</th><th>baths</th><th>parking</th><th>property_type</th></tr>\n",
       "<tr><td>https://www.domai...</td><td>3000</td><td>melbourne</td><td>4307/639 Little L...</td><td>$600 and Fully Fu...</td><td>1</td><td>1</td><td>0</td><td>Apartment / Unit ...</td></tr>\n",
       "<tr><td>https://www.domai...</td><td>3000</td><td>melbourne</td><td>2213/27 Little Co...</td><td>$750 a week and F...</td><td>2</td><td>2</td><td>0</td><td>Apartment / Unit ...</td></tr>\n",
       "<tr><td>https://www.domai...</td><td>3000</td><td>melbourne</td><td>4207/371 Little L...</td><td>$720 per week opp...</td><td>2</td><td>1</td><td>0</td><td>Apartment / Unit ...</td></tr>\n",
       "<tr><td>https://www.domai...</td><td>3000</td><td>melbourne</td><td>99 Franklin Stree...</td><td>Furnished, bills,...</td><td>1</td><td>1</td><td>0</td><td>Apartment / Unit ...</td></tr>\n",
       "<tr><td>https://www.domai...</td><td>3000</td><td>melbourne</td><td>1302/279-283 La T...</td><td>$650 and Fully Fu...</td><td>2</td><td>1</td><td>0</td><td>Apartment / Unit ...</td></tr>\n",
       "<tr><td>https://www.domai...</td><td>3000</td><td>melbourne</td><td>103/19 Exploratio...</td><td>$540 Per Week Inc...</td><td>1</td><td>1</td><td>0</td><td>Studio</td></tr>\n",
       "<tr><td>https://www.domai...</td><td>3000</td><td>melbourne</td><td>913/22-24 Jane Be...</td><td>$520 and Fully Fu...</td><td>1</td><td>1</td><td>1</td><td>Apartment / Unit ...</td></tr>\n",
       "<tr><td>https://www.domai...</td><td>3000</td><td>melbourne</td><td>1202/601 Little C...</td><td>$620 per week, $2...</td><td>2</td><td>1</td><td>0</td><td>Apartment / Unit ...</td></tr>\n",
       "<tr><td>https://www.domai...</td><td>3000</td><td>melbourne</td><td>4211/371 Little L...</td><td>$750 Per Week</td><td>2</td><td>1</td><td>0</td><td>Apartment / Unit ...</td></tr>\n",
       "<tr><td>https://www.domai...</td><td>3000</td><td>melbourne</td><td>1308/138 Spencer ...</td><td>$800 and Fully Fu...</td><td>2</td><td>2</td><td>0</td><td>Apartment / Unit ...</td></tr>\n",
       "<tr><td>https://www.domai...</td><td>3000</td><td>melbourne</td><td>4311/371 Little L...</td><td>$850 per week &amp; A...</td><td>2</td><td>1</td><td>0</td><td>Apartment / Unit ...</td></tr>\n",
       "<tr><td>https://www.domai...</td><td>3000</td><td>melbourne</td><td>3506/228 La Trobe...</td><td>$650 a week and F...</td><td>1</td><td>1</td><td>0</td><td>Apartment / Unit ...</td></tr>\n",
       "<tr><td>https://www.domai...</td><td>3000</td><td>melbourne</td><td>236 La Trobe Stre...</td><td>Furnished, all in...</td><td>1</td><td>1</td><td>0</td><td>Apartment / Unit ...</td></tr>\n",
       "<tr><td>https://www.domai...</td><td>3000</td><td>melbourne</td><td>Level 4, 401B/120...</td><td>$860 furnished/3 ...</td><td>2</td><td>1</td><td>0</td><td>Apartment / Unit ...</td></tr>\n",
       "<tr><td>https://www.domai...</td><td>3000</td><td>melbourne</td><td>405/151 Berkeley ...</td><td>$720 **OPPOSITE T...</td><td>2</td><td>2</td><td>0</td><td>Apartment / Unit ...</td></tr>\n",
       "<tr><td>https://www.domai...</td><td>3000</td><td>melbourne</td><td>4703/568 Collins ...</td><td>$750 a week and F...</td><td>2</td><td>2</td><td>1</td><td>Apartment / Unit ...</td></tr>\n",
       "<tr><td>https://www.domai...</td><td>3000</td><td>melbourne</td><td>1006/442 Elizabet...</td><td>$650 per week and...</td><td>1</td><td>1</td><td>0</td><td>Apartment / Unit ...</td></tr>\n",
       "<tr><td>https://www.domai...</td><td>3000</td><td>melbourne</td><td>1406/22 Jane Bell...</td><td>$795 include bill...</td><td>2</td><td>1</td><td>1</td><td>Apartment / Unit ...</td></tr>\n",
       "<tr><td>https://www.domai...</td><td>3000</td><td>melbourne</td><td>2514/23 Mackenzie...</td><td>$720 per week *Pa...</td><td>2</td><td>1</td><td>0</td><td>Apartment / Unit ...</td></tr>\n",
       "<tr><td>https://www.domai...</td><td>3000</td><td>melbourne</td><td>4302/120 A'Becket...</td><td>$1200 **INSPECTIO...</td><td>3</td><td>2</td><td>0</td><td>Apartment / Unit ...</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+--------+---------+--------------------+--------------------+----+-----+-------+--------------------+\n",
       "|                 url|postcode|   suburb|                name|           cost_text|beds|baths|parking|       property_type|\n",
       "+--------------------+--------+---------+--------------------+--------------------+----+-----+-------+--------------------+\n",
       "|https://www.domai...|    3000|melbourne|4307/639 Little L...|$600 and Fully Fu...|   1|    1|      0|Apartment / Unit ...|\n",
       "|https://www.domai...|    3000|melbourne|2213/27 Little Co...|$750 a week and F...|   2|    2|      0|Apartment / Unit ...|\n",
       "|https://www.domai...|    3000|melbourne|4207/371 Little L...|$720 per week opp...|   2|    1|      0|Apartment / Unit ...|\n",
       "|https://www.domai...|    3000|melbourne|99 Franklin Stree...|Furnished, bills,...|   1|    1|      0|Apartment / Unit ...|\n",
       "|https://www.domai...|    3000|melbourne|1302/279-283 La T...|$650 and Fully Fu...|   2|    1|      0|Apartment / Unit ...|\n",
       "|https://www.domai...|    3000|melbourne|103/19 Exploratio...|$540 Per Week Inc...|   1|    1|      0|              Studio|\n",
       "|https://www.domai...|    3000|melbourne|913/22-24 Jane Be...|$520 and Fully Fu...|   1|    1|      1|Apartment / Unit ...|\n",
       "|https://www.domai...|    3000|melbourne|1202/601 Little C...|$620 per week, $2...|   2|    1|      0|Apartment / Unit ...|\n",
       "|https://www.domai...|    3000|melbourne|4211/371 Little L...|       $750 Per Week|   2|    1|      0|Apartment / Unit ...|\n",
       "|https://www.domai...|    3000|melbourne|1308/138 Spencer ...|$800 and Fully Fu...|   2|    2|      0|Apartment / Unit ...|\n",
       "|https://www.domai...|    3000|melbourne|4311/371 Little L...|$850 per week & A...|   2|    1|      0|Apartment / Unit ...|\n",
       "|https://www.domai...|    3000|melbourne|3506/228 La Trobe...|$650 a week and F...|   1|    1|      0|Apartment / Unit ...|\n",
       "|https://www.domai...|    3000|melbourne|236 La Trobe Stre...|Furnished, all in...|   1|    1|      0|Apartment / Unit ...|\n",
       "|https://www.domai...|    3000|melbourne|Level 4, 401B/120...|$860 furnished/3 ...|   2|    1|      0|Apartment / Unit ...|\n",
       "|https://www.domai...|    3000|melbourne|405/151 Berkeley ...|$720 **OPPOSITE T...|   2|    2|      0|Apartment / Unit ...|\n",
       "|https://www.domai...|    3000|melbourne|4703/568 Collins ...|$750 a week and F...|   2|    2|      1|Apartment / Unit ...|\n",
       "|https://www.domai...|    3000|melbourne|1006/442 Elizabet...|$650 per week and...|   1|    1|      0|Apartment / Unit ...|\n",
       "|https://www.domai...|    3000|melbourne|1406/22 Jane Bell...|$795 include bill...|   2|    1|      1|Apartment / Unit ...|\n",
       "|https://www.domai...|    3000|melbourne|2514/23 Mackenzie...|$720 per week *Pa...|   2|    1|      0|Apartment / Unit ...|\n",
       "|https://www.domai...|    3000|melbourne|4302/120 A'Becket...|$1200 **INSPECTIO...|   3|    2|      0|Apartment / Unit ...|\n",
       "+--------------------+--------+---------+--------------------+--------------------+----+-----+-------+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = suburbs_df[suburbs_df['postcode'] >= 3950]\n",
    "chunk_dict['chunk_3997'] = temp[temp['postcode'] < 3997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode = list(range(3001, 4000))  # List of postcodes\n",
    "chunk_size = 50  # Define the chunk size\n",
    "\n",
    "# Loop over the postcodes in chunks of 50\n",
    "for i in range(0, len(postcode), chunk_size):\n",
    "    # Extract a chunk of 50 postcodes\n",
    "    chunk = postcode[i:i + chunk_size]\n",
    "    \n",
    "    # Convert chunk to string or appropriate format for your function\n",
    "    chunk_name = f'chunk_{i // chunk_size + 1}'\n",
    "    \n",
    "    # Call start_scrape function with the chunk\n",
    "    property_metadata = start_scrape(chunk_dict[chunk_name], f'{chunk}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/davynr06/Documents/MAST30034/project-2-group-real-estate-industry-project-22/notebooks/Working.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/davynr06/Documents/MAST30034/project-2-group-real-estate-industry-project-22/notebooks/Working.ipynb#ch0000012?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m chunk_dict[\u001b[39m1\u001b[39;49m:]:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davynr06/Documents/MAST30034/project-2-group-real-estate-industry-project-22/notebooks/Working.ipynb#ch0000012?line=1'>2</a>\u001b[0m     start_scrape(chunk_dict[i])\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "for i in chunk_dict[1:]:\n",
    "    start_scrape(chunk_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/Users/davynr06/Documents/MAST30034/project-2-group-real-estate-industry-project-22/data/raw/work_3050.json.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/davynr06/Documents/MAST30034/project-2-group-real-estate-industry-project-22/notebooks/Working.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davynr06/Documents/MAST30034/project-2-group-real-estate-industry-project-22/notebooks/Working.ipynb#ch0000013?line=0'>1</a>\u001b[0m schema \u001b[39m=\u001b[39m StructType([\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davynr06/Documents/MAST30034/project-2-group-real-estate-industry-project-22/notebooks/Working.ipynb#ch0000013?line=1'>2</a>\u001b[0m         StructField(\u001b[39m\"\u001b[39m\u001b[39murl\u001b[39m\u001b[39m\"\u001b[39m, StringType(), \u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davynr06/Documents/MAST30034/project-2-group-real-estate-industry-project-22/notebooks/Working.ipynb#ch0000013?line=2'>3</a>\u001b[0m         StructField(\u001b[39m\"\u001b[39m\u001b[39mpostcode\u001b[39m\u001b[39m\"\u001b[39m, StringType(), \u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davynr06/Documents/MAST30034/project-2-group-real-estate-industry-project-22/notebooks/Working.ipynb#ch0000013?line=9'>10</a>\u001b[0m         StructField(\u001b[39m\"\u001b[39m\u001b[39mproperty_type\u001b[39m\u001b[39m\"\u001b[39m, StringType(), \u001b[39mTrue\u001b[39;00m),  \u001b[39m# Property type field\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davynr06/Documents/MAST30034/project-2-group-real-estate-industry-project-22/notebooks/Working.ipynb#ch0000013?line=10'>11</a>\u001b[0m     ])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/davynr06/Documents/MAST30034/project-2-group-real-estate-industry-project-22/notebooks/Working.ipynb#ch0000013?line=11'>12</a>\u001b[0m work \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mschema(schema)\u001b[39m.\u001b[39;49mjson(\u001b[39m'\u001b[39;49m\u001b[39m../data/raw/work_3050.json\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/sql/readwriter.py:425\u001b[0m, in \u001b[0;36mDataFrameReader.json\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/sql/readwriter.py?line=422'>423</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/sql/readwriter.py?line=423'>424</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/sql/readwriter.py?line=424'>425</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mjson(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spark\u001b[39m.\u001b[39;49m_sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonUtils\u001b[39m.\u001b[39;49mtoSeq(path)))\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/sql/readwriter.py?line=425'>426</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/sql/readwriter.py?line=427'>428</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(iterator: Iterable) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterable:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/py4j/java_gateway.py?line=1315'>1316</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/py4j/java_gateway.py?line=1316'>1317</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/py4j/java_gateway.py?line=1317'>1318</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/py4j/java_gateway.py?line=1318'>1319</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/py4j/java_gateway.py?line=1320'>1321</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/py4j/java_gateway.py?line=1321'>1322</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/py4j/java_gateway.py?line=1322'>1323</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/py4j/java_gateway.py?line=1324'>1325</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/py4j/java_gateway.py?line=1325'>1326</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py?line=180'>181</a>\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py?line=181'>182</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py?line=182'>183</a>\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py?line=183'>184</a>\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py?line=184'>185</a>\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py?line=185'>186</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py?line=186'>187</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/Users/davynr06/Documents/MAST30034/project-2-group-real-estate-industry-project-22/data/raw/work_3050.json."
     ]
    }
   ],
   "source": [
    "\n",
    "schema = StructType([\n",
    "        StructField(\"url\", StringType(), True),\n",
    "        StructField(\"postcode\", StringType(), True),\n",
    "        StructField(\"suburb\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"cost_text\", StringType(), True),\n",
    "        StructField(\"beds\", StringType(), True),  # Separate field for beds\n",
    "        StructField(\"baths\", StringType(), True),  # Separate field for baths\n",
    "        StructField(\"parking\", StringType(), True),  # Parking field\n",
    "        StructField(\"property_type\", StringType(), True),  # Property type field\n",
    "    ])\n",
    "work = spark.read.schema(schema).json('../data/raw/work_3050.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+----+---------+----+-----+-------+-------------+\n",
      "|url|postcode|suburb|name|cost_text|beds|baths|parking|property_type|\n",
      "+---+--------+------+----+---------+----+-----+-------+-------------+\n",
      "+---+--------+------+----+---------+----+-----+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "work.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  url  postcode   suburb  \\\n",
      "0   https://www.domain.com.au/34-evadene-drive-tar...      3029  tarneit   \n",
      "1   https://www.domain.com.au/434-bethany-road-tar...      3029  tarneit   \n",
      "2   https://www.domain.com.au/58-antonio-road-tarn...      3029  tarneit   \n",
      "3   https://www.domain.com.au/12-lindeman-street-t...      3029  tarneit   \n",
      "4   https://www.domain.com.au/3-imatra-loop-tarnei...      3029  tarneit   \n",
      "5   https://www.domain.com.au/84-lucania-crescent-...      3029  tarneit   \n",
      "6   https://www.domain.com.au/8-keeping-terrace-ta...      3029  tarneit   \n",
      "7   https://www.domain.com.au/40-kamala-drive-tarn...      3029  tarneit   \n",
      "8   https://www.domain.com.au/48-riland-boulevard-...      3029  tarneit   \n",
      "9   https://www.domain.com.au/9-ceremony-drive-tar...      3029  tarneit   \n",
      "10  https://www.domain.com.au/11-ogawa-walk-tarnei...      3029  tarneit   \n",
      "11  https://www.domain.com.au/tarneit-vic-3029-151...      3029  tarneit   \n",
      "\n",
      "                                     name                 cost_text  beds  \\\n",
      "0      34 Evadene Drive, Tarneit VIC 3029             $620 per week     4   \n",
      "1      434 Bethany Road, Tarneit VIC 3029                      $520     3   \n",
      "2       58 Antonio Road, Tarneit VIC 3029             $580 Per Week     4   \n",
      "3    12 Lindeman Street, Tarneit VIC 3029                   $650 pw     4   \n",
      "4         3 Imatra Loop, Tarneit VIC 3029                      $570     4   \n",
      "5   84 Lucania Crescent, Tarneit VIC 3029             Contact Agent     4   \n",
      "6     8 Keeping Terrace, Tarneit VIC 3029            $ 500 PER WEEK     3   \n",
      "7       40 Kamala Drive, Tarneit VIC 3029                      $520     3   \n",
      "8   48 Riland Boulevard, Tarneit VIC 3029                      $520     3   \n",
      "9      9 Ceremony Drive, Tarneit VIC 3029                       610     4   \n",
      "10        11 Ogawa Walk, Tarneit VIC 3029                       530     3   \n",
      "11                       Tarneit VIC 3029  Rent2own with No Deposit     4   \n",
      "\n",
      "    baths  parking property_type  \n",
      "0       3        2         House  \n",
      "1       2        2         House  \n",
      "2       2        2         House  \n",
      "3       2        2         House  \n",
      "4       2        2         House  \n",
      "5       2        2         House  \n",
      "6       2        1         House  \n",
      "7       2        1         House  \n",
      "8       2        1         House  \n",
      "9       2        2         House  \n",
      "10      2        2         House  \n",
      "11      2        2         House  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the folder path containing JSON files\n",
    "folder_path = '../data/raw/work.json'\n",
    "\n",
    "# List all files in the directory\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Read each JSON file into a DataFrame\n",
    "for file in json_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    # Read JSON file\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "    # Append the DataFrame to the list\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(combined_df.head(12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run these code chunks after first running create_chunk(), start_scrape() and run_chunk() methods: \\\n",
    "1st Cell: Davyn \\\n",
    "2nd Cell: Arpan \\\n",
    "3rd Cell: Priscilla \\\n",
    "4th Cell: Rachel \\\n",
    "5th Cell: Nathan \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chunk(starting_chunk):\n",
    "    i = starting_chunk\n",
    "    # we are running chunks of 25 postcodes 7 times each\n",
    "    while i < starting_chunk + 175:\n",
    "        start_scrape(chunk_dict[\"chunk_{}\".format(i)], i) #i.split(\"_\")[1])\n",
    "        i += 25\n",
    "    if i == 3923:\n",
    "        temp = suburbs_df[suburbs_df['postcode'] >= i + 1]\n",
    "        chunk_dict['chunk_3996'] = temp[temp['postcode'] < 3997]\n",
    "        start_scrape(chunk_dict['chunk_3996'], 3996)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Davyn\n",
    "starting_chunk = 3048\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arpan\n",
    "starting_chunk = 3048 + 175\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Priscilla\n",
    "starting_chunk = 3048 + 350\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rachel\n",
    "starting_chunk = 3048 + 525\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for archerton (3723)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1008810/4257810455.py:69: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  no_results = bs_object.find(text=re.compile(\"No results found\", re.I))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more results for archerton on page 1.\n",
      "No results for archerton. Moving to the next suburb.\n",
      "Scraping data for barjarg (3723)\n",
      "No more results for barjarg on page 1.\n",
      "No results for barjarg. Moving to the next suburb.\n",
      "Scraping data for boorolite (3723)\n",
      "No more results for boorolite on page 1.\n",
      "No results for boorolite. Moving to the next suburb.\n",
      "Scraping data for bridge-creek (3723)\n",
      "No more results for bridge-creek on page 1.\n",
      "No results for bridge-creek. Moving to the next suburb.\n",
      "Scraping data for delatite (3723)\n",
      "No more results for delatite on page 1.\n",
      "No results for delatite. Moving to the next suburb.\n",
      "Scraping data for enochs-point (3723)\n",
      "No more results for enochs-point on page 1.\n",
      "No results for enochs-point. Moving to the next suburb.\n",
      "Scraping data for gaffneys-creek (3723)\n",
      "No more results for gaffneys-creek on page 1.\n",
      "No results for gaffneys-creek. Moving to the next suburb.\n",
      "Scraping data for goughs-bay (3723)\n",
      "No more results for goughs-bay on page 1.\n",
      "No results for goughs-bay. Moving to the next suburb.\n",
      "Scraping data for howes-creek (3723)\n",
      "No more results for howes-creek on page 1.\n",
      "No results for howes-creek. Moving to the next suburb.\n",
      "Scraping data for howqua (3723)\n",
      "No more results for howqua on page 1.\n",
      "No results for howqua. Moving to the next suburb.\n",
      "Scraping data for howqua-hills (3723)\n",
      "No more results for howqua-hills on page 1.\n",
      "No results for howqua-hills. Moving to the next suburb.\n",
      "Scraping data for howqua-inlet (3723)\n",
      "No more results for howqua-inlet on page 1.\n",
      "No results for howqua-inlet. Moving to the next suburb.\n",
      "Scraping data for jamieson (3723)\n",
      "No more results for jamieson on page 1.\n",
      "No results for jamieson. Moving to the next suburb.\n",
      "Scraping data for kevington (3723)\n",
      "No more results for kevington on page 1.\n",
      "No results for kevington. Moving to the next suburb.\n",
      "Scraping data for knockwood (3723)\n",
      "No more results for knockwood on page 1.\n",
      "No results for knockwood. Moving to the next suburb.\n",
      "Scraping data for macs-cove (3723)\n",
      "No more results for macs-cove on page 1.\n",
      "No results for macs-cove. Moving to the next suburb.\n",
      "Scraping data for maindample (3723)\n",
      "No more results for maindample on page 1.\n",
      "No results for maindample. Moving to the next suburb.\n",
      "Scraping data for matlock (3723)\n",
      "No more results for matlock on page 1.\n",
      "No results for matlock. Moving to the next suburb.\n",
      "Scraping data for merrijig (3723)\n",
      "No more results for merrijig on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 2/2 [00:01<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for mount-buller (3723)\n",
      "No more results for mount-buller on page 1.\n",
      "No results for mount-buller. Moving to the next suburb.\n",
      "Scraping data for mountain-bay (3723)\n",
      "No more results for mountain-bay on page 1.\n",
      "No results for mountain-bay. Moving to the next suburb.\n",
      "Scraping data for nillahcootie (3723)\n",
      "Error fetching https://www.domain.com.au/rent/nillahcootie-vic-3723/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for nillahcootie. Moving to the next suburb.\n",
      "Scraping data for piries (3723)\n",
      "No more results for piries on page 1.\n",
      "No results for piries. Moving to the next suburb.\n",
      "Scraping data for sawmill-settlement (3723)\n",
      "No more results for sawmill-settlement on page 1.\n",
      "No results for sawmill-settlement. Moving to the next suburb.\n",
      "Scraping data for tolmie (3723)\n",
      "No more results for tolmie on page 1.\n",
      "No results for tolmie. Moving to the next suburb.\n",
      "Scraping data for woods-point (3723)\n",
      "No more results for woods-point on page 1.\n",
      "No results for woods-point. Moving to the next suburb.\n",
      "Scraping data for mansfield (3724)\n",
      "Error fetching https://www.domain.com.au/rent/mansfield-vic-3724/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for mansfield. Moving to the next suburb.\n",
      "Scraping data for boxwood (3725)\n",
      "No more results for boxwood on page 1.\n",
      "No results for boxwood. Moving to the next suburb.\n",
      "Scraping data for chesney-vale (3725)\n",
      "No more results for chesney-vale on page 1.\n",
      "No results for chesney-vale. Moving to the next suburb.\n",
      "Scraping data for goorambat (3725)\n",
      "No more results for goorambat on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for major-plains (3725)\n",
      "No more results for major-plains on page 1.\n",
      "No results for major-plains. Moving to the next suburb.\n",
      "Scraping data for stewarton (3725)\n",
      "No more results for stewarton on page 1.\n",
      "No results for stewarton. Moving to the next suburb.\n",
      "Scraping data for bungeet (3726)\n",
      "No more results for bungeet on page 1.\n",
      "No results for bungeet. Moving to the next suburb.\n",
      "Scraping data for bungeet-west (3726)\n",
      "No more results for bungeet-west on page 1.\n",
      "No results for bungeet-west. Moving to the next suburb.\n",
      "Scraping data for devenish (3726)\n",
      "No more results for devenish on page 1.\n",
      "No results for devenish. Moving to the next suburb.\n",
      "Scraping data for thoona (3726)\n",
      "No more results for thoona on page 1.\n",
      "No results for thoona. Moving to the next suburb.\n",
      "Scraping data for almonds (3727)\n",
      "No more results for almonds on page 1.\n",
      "No results for almonds. Moving to the next suburb.\n",
      "Scraping data for lake-rowan (3727)\n",
      "No more results for lake-rowan on page 1.\n",
      "No results for lake-rowan. Moving to the next suburb.\n",
      "Scraping data for pelluebla (3727)\n",
      "No more results for pelluebla on page 1.\n",
      "No results for pelluebla. Moving to the next suburb.\n",
      "Scraping data for st-james (3727)\n",
      "No more results for st-james on page 1.\n",
      "No results for st-james. Moving to the next suburb.\n",
      "Scraping data for waggarandall (3727)\n",
      "Error fetching https://www.domain.com.au/rent/waggarandall-vic-3727/?ssubs=0&sort=suburb-asc&page=1: HTTP Error 404: Not Found\n",
      "No results for waggarandall. Moving to the next suburb.\n",
      "Scraping data for yundool (3727)\n",
      "No more results for yundool on page 1.\n",
      "No results for yundool. Moving to the next suburb.\n",
      "Scraping data for boomahnoomoonah (3728)\n",
      "No more results for boomahnoomoonah on page 1.\n",
      "No results for boomahnoomoonah. Moving to the next suburb.\n",
      "Scraping data for tungamah (3728)\n",
      "No more results for tungamah on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for wilby (3728)\n",
      "No more results for wilby on page 1.\n",
      "No results for wilby. Moving to the next suburb.\n",
      "Scraping data for youarang (3728)\n",
      "No more results for youarang on page 1.\n",
      "No results for youarang. Moving to the next suburb.\n",
      "Scraping data for bathumi (3730)\n",
      "No more results for bathumi on page 1.\n",
      "No results for bathumi. Moving to the next suburb.\n",
      "Scraping data for boosey (3730)\n",
      "No more results for boosey on page 1.\n",
      "No results for boosey. Moving to the next suburb.\n",
      "Scraping data for bundalong (3730)\n",
      "No more results for bundalong on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful: 100%|██████████| 2/2 [00:01<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for bundalong-south (3730)\n",
      "No more results for bundalong-south on page 1.\n",
      "No results for bundalong-south. Moving to the next suburb.\n",
      "Scraping data for burramine (3730)\n",
      "No more results for burramine on page 1.\n",
      "No results for burramine. Moving to the next suburb.\n",
      "Scraping data for burramine-south (3730)\n",
      "No more results for burramine-south on page 1.\n",
      "No results for burramine-south. Moving to the next suburb.\n",
      "Scraping data for esmond (3730)\n",
      "No more results for esmond on page 1.\n",
      "No results for esmond. Moving to the next suburb.\n",
      "Scraping data for telford (3730)\n",
      "No more results for telford on page 1.\n",
      "No results for telford. Moving to the next suburb.\n",
      "Scraping data for yarrawonga (3730)\n",
      "No more results for yarrawonga on page 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% successful:  39%|███▉      | 7/18 [00:03<00:05,  1.97it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Nathan\u001b[39;00m\n\u001b[1;32m      2\u001b[0m starting_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3048\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m700\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrun_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstarting_chunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 5\u001b[0m, in \u001b[0;36mrun_chunk\u001b[0;34m(starting_chunk)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# we are running chunks of 25 postcodes 7 times each\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m starting_chunk \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m175\u001b[39m:\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mstart_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#i.split(\"_\")[1])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3923\u001b[39m:\n",
      "Cell \u001b[0;32mIn[35], line 102\u001b[0m, in \u001b[0;36mstart_scrape\u001b[0;34m(chunk, file_suffix)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m property_url \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m         bs_object \u001b[38;5;241m=\u001b[39m BeautifulSoup(\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproperty_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUser-Agent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPostmanRuntime/7.6.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m         total_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;66;03m# Get property name\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:1352\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[0;32m-> 1352\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1354\u001b[0m     h\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Nathan\n",
    "starting_chunk = 3048 + 700\n",
    "run_chunk(starting_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3398"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3048+175 + 175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_3048\n",
      "chunk_3073\n",
      "chunk_3098\n",
      "chunk_3123\n",
      "chunk_3148\n",
      "chunk_3173\n",
      "chunk_3198\n",
      "chunk_3223\n",
      "chunk_3248\n",
      "chunk_3273\n",
      "chunk_3298\n",
      "chunk_3323\n",
      "chunk_3348\n",
      "chunk_3373\n",
      "chunk_3398\n",
      "chunk_3423\n",
      "chunk_3448\n",
      "chunk_3473\n",
      "chunk_3498\n",
      "chunk_3523\n",
      "chunk_3548\n",
      "chunk_3573\n",
      "chunk_3598\n",
      "chunk_3623\n",
      "chunk_3648\n",
      "chunk_3673\n",
      "chunk_3698\n",
      "chunk_3723\n",
      "chunk_3748\n",
      "chunk_3773\n",
      "chunk_3798\n",
      "chunk_3823\n",
      "chunk_3848\n",
      "chunk_3873\n",
      "chunk_3898\n",
      "chunk_3923\n",
      "chunk_3948\n",
      "chunk_3973\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
